---
title: "Port Everglades Coral Survey Data Analysis"
author: "R. Cunning"
date: "2025-04-21"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2      
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
```

# Setup
```{r libraries}
# Load packages
library(sf)
library(xml2)
library(tidyverse)
library(tidybayes)
library(brms)
library(vegan)
library(kableExtra)
library(treemapify)
library(ggrepel)
library(igraph)
library(cowplot)
```

```{r custom}
# Define genus level taxon groups (plus one family FAVI)
taxon_groups <- list(
  PORI = c("PPOR", "PFUR", "PDIV", "PAST", "PORI"),
  ORBI = c("OFAV", "OANN", "OFRA", "ORBI"),
  AGAR = c("AFRA", "AAGA", "AHUM", "ALAM", "AGAR"),
  MADR = c("MAUR", "MSEN", "MDEC", "MPHA", "MADR"),
  SOLE = c("SHYA", "SBOU", "SOLE"),
  SCOL = c("SLAC", "SCUB", "SCOL"),
  SIDE = c("SSID", "SRAD", "SIDE"),
  MYCE = c("MFER", "MLAM", "MALI", "MYCE"),
  OCUL = c("OROB", "ODIF", "OCUL")
)

# Convert to lookup tibble
taxon_lookup <- enframe(taxon_groups, name = "taxon_group", value = "taxon") %>%
  unnest(taxon)



# Define juvenile family level taxon groups (following DRM survey convention)
taxon_groups_juv <- list(
  MUSS = c("ISIN", "ISOP", "MANG", "MYCE", "SCOL", "MUSS", "MALI", "MFER"),
  FAVI = c("FAVI", "FFRA", "MARE", "DLAB", "PSTR", "PCLI", "CNAT"),
  MEAN = c("MMEA", "MEAN", "DCYL", "DSTO", "EFAS")
)
# Convert to lookup tibble
taxon_lookup_juv <- enframe(taxon_groups_juv, name = "taxon_group", value = "taxon") %>%
  unnest(taxon)


# Create taxon labels
taxon_labels <- c(
  ACER = 'italic("A. cervicornis")',
  AGAR = 'italic("Agaricia")~spp.',
  CNAT = 'italic("C. natans")',
  DLAB = 'italic("D. labyrinthiformis")',
  DSTO = 'italic("D. stokesii")',
  EFAS = 'italic("E. fastigiata")',
  FAVI = '"Faviinae"',
  MADR = 'italic("Madracis")~spp.',
  MCAV = 'italic("M. cavernosa")',
  MEAN = '"Meandrinidae"',
  MMEA = 'italic("M. meandrites")',
  MUSS = '"Mussinae"',
  MYCE = 'italic("Mycetophyllia")~spp.',
  ORBI = 'italic("Orbicella")~spp.',
  PCLI = 'italic("P. clivosa")',
  PORI = 'italic("Porites")~spp.',
  PSTR = 'italic("P. strigosa")',
  SIDE = 'italic("Siderastrea")~spp.',
  SINT = 'italic("S. intersepta")',
  SOLE = 'italic("Solenastrea")~spp.'
)



# Order levels of Habitat Types
type_levels <- c("Nearshore Ridge Complex", "Inner Reef", "Middle Reef",
                 "Outer Reef")
type_labels <- c("NRC", "IR", "MR", "OR")
names(type_labels) <- type_levels

# Order survey datasets
dataset_levels <- c("nsu11_esa", "dca17_esa", "dca17", "tt21", "tt23", "drm24")
dataset_labels <- c("2011-NSU-ESA", "2017-DCA-ESA", "2017—DCA", "2021—TT", "2023—TT", "2024—Shedd")
names(dataset_labels) <- dataset_levels
```

# Import data

## 2011 NSU ESA Survey
```{r}
# Import data
nsu11_esa0 <- readxl::read_xlsx("data/2011_nsu_esa/Port Everglades_NSU 2011and DCA 2017_ESA surveys.xlsx",
                           sheet = "2011_NSU_ESA survey") %>%
  janitor::clean_names()

# Site metadata
nsu11_esa_sitemd <- nsu11_esa0 %>%
  select(site = ident, latitude = lat, longitude = long) %>%
  mutate(site = as.character(site))



# ESA coral count data
## DO NOT KEEP M.FEROX DATA -- ALL OTHER SURVEYS ALL MYCETOPHYLLIA ARE ALICIAE, so cannot combine at 'MYCE' (genus) level
nsu11_esa_counts <- nsu11_esa0 %>%
  select(site = ident,
         ACER = a_cervic_1, OANN = m_annula_1, OFAV = m_faveol_1, OFRA = m_franks_1, DSTO = d_stokes_1) %>%
  pivot_longer(-site, names_to = "taxon", values_to = "n") %>%
  # Assume all were >4cm since no sizes are reported
  mutate(class = ">4cm") %>%
  mutate(site = as.character(site))

# Aggregate taxa (multiple orbicella observed --> ORBI)
nsu11_esa_counts_ag <- nsu11_esa_counts %>%
  left_join(taxon_lookup, by = "taxon") %>%
  mutate(taxon = coalesce(taxon_group, taxon)) %>%
  select(-taxon_group) %>%
  group_by(across(-n)) %>%
  summarize(n = sum(n), .groups = "drop")
  
```

## 2017 DCA ESA Survey
```{r}
# # 163 sites in this list
# dca17esaall <- readxl::read_xlsx("data/2017_dca_esa/Appendix C_DCA2017_survey_results_FINAL.xlsx") %>%
#   janitor::clean_names() %>%
#   mutate(site = esa_site)
# library(sf)
# 
# # Read KML file
# dca17_esa_sitemd0 <- st_read("data/2017_dca_esa/Listed_Coral_Survey_2017.kml")
# # contains 149 sites
# 
# 
# # Extract coordinates and save as CSV
# dca17_esa_sitemd <- dca17_esa_sitemd0 %>%
#   mutate(site = Name,
#          longitude = st_coordinates(.)[, 1],
#          latitude = st_coordinates(.)[, 2]) %>%
#   st_drop_geometry() %>%
#   select(site, longitude, latitude) %>%
#   as_tibble() %>%
#   mutate(site = as.character(site))
# # only has 149 sites.
# ### MISSING SITES?
# 
# setdiff(dca17esaall$site, dca17_esa_sitemd$site)


# New sitemd
dca17_esa_sitemd0 <- readxl::read_xlsx("data/2017_dca_esa/Port Everglades_NSU 2011and DCA 2017_ESA surveys.xlsx", 
                                       sheet = "2017_DCA_ESA survey") %>% janitor::clean_names()

dca17_esa_sitemd <- dca17_esa_sitemd0 %>%
  select(site = id, longitude, latitude) %>%
  mutate(site = as.character(site))

# Get coral data


# Appendix D -- contains individual colony sizes for observed corals
# what is the "NUMBER" column? ignore for now.
dca17_esa0 <- readxl::read_xlsx("data/2017_dca_esa/Appendix D_ESA_listed_coral_Plotted_Locations.xlsx") %>%
  janitor::clean_names() %>%
  select(site = site_id, taxon = species, length_cm, width_cm, height_cm, m2_of_habi, density) %>%
  mutate(across(ends_with("cm"), as.numeric))

# Assign size classes
dca17_esa <- dca17_esa0 %>%
  mutate(max_dim_cm = pmax(length_cm, width_cm, height_cm, na.rm = TRUE),
         class = if_else(max_dim_cm >= 4, ">4cm", "<4cm"))

# Count taxon and size class per site
dca17_esa_counts <- dca17_esa %>%
  count(site, taxon, class) %>%
  mutate(site = as.character(site))
n_distinct(dca17_esa_counts$site)    # 57 sites had counts > 0 (where are zeros recorded?) 


# Total area surveyed per site = 784 m2 (crossed 100x4m belt transects with 16m2 overlap)

# Agreggate taxa (OFAV -> ORBI)
dca17_esa_counts_ag <- dca17_esa_counts %>%
  left_join(taxon_lookup, by = "taxon") %>%
  mutate(taxon = coalesce(taxon_group, taxon)) %>%
  select(-taxon_group) %>%
  group_by(across(-n)) %>%
  summarize(n = sum(n), .groups = "drop")


# Merge with site metadata to get zero counts
# Step 1: Define all taxon-class combos you want to include
# This can be pulled from your full dataset (like dfftaxon), or hardcoded:
all_taxon_class <- expand_grid(
  taxon = c("ACER", "ORBI"),  # example taxa
  class = c("<4cm", ">4cm")
)

# Step 2: Get all site IDs from the site metadata
all_sites <- dca17_esa_sitemd %>%
  distinct(site)

# Step 3: Create full site × taxon × class combinations
full_grid <- expand_grid(
  site = as.character(all_sites$site),
  all_taxon_class
)

# Step 4: Left join with observed counts and fill missing with 0
dca17_esa_counts_ag_full <- full_grid %>%
  left_join(dca17_esa_counts_ag, by = c("site", "taxon", "class")) %>%
  mutate(n = replace_na(n, 0))
```


## 2017 DCA Recon Survey

DCA site metadata
```{r 2017_DCA_site_metadata}
# Site metadata
# Read in site coordinates
dca17_sitemd0 <- readxl::read_xlsx("data/2017_dca_recon/Recon_Site_Coordinates_Extracted.xlsx") %>%
  janitor::clean_names()

# All sites have start and end coordinates...

# Tidy and Calculate midpoint per transect
dca17_sitemd <- dca17_sitemd0 %>%
  mutate(
    depth = abs(as.numeric(depth)),
    across(c(latitude, longitude), as.numeric)
  ) %>%
  group_by(site = transect) %>%
  summarize(
    latitude = mean(latitude, na.rm = TRUE),
    longitude = mean(longitude, na.rm = TRUE),
    depth = mean(depth, na.rm = TRUE),
    .groups = "drop"
  )
```

DCA coral data
```{r 2017_DCA_coral_data}
# Read in survey data
dca170 <- readxl::read_xlsx("data/2017_dca_recon/Compiled_DCA_RECON_Belt_data.xlsx") %>%
  janitor::clean_names()

dca17 <- dca170 %>%
  select(1:18) %>%
  rename(site = site_name) %>%
  mutate(site = factor(site)) %>%
  select(site, taxon = coral_species, max_width_cm = max_size_cm)

# Adjust/corrects species IDs
sort(unique(dca17$taxon))
dca17 <- dca17 %>%
  mutate(taxon = case_when(
    taxon == "AGA SP" ~ "AGAR",
    taxon == "LCUC" ~ "HCUC",
    taxon %in% c("MYCSP", "Mycetophyllia spp.") ~ "MYCE",
    taxon == "OFAV\\" ~ "OFAV",
    taxon %in% c("MAD SP", "MADSP") ~ "MADR",
    taxon == "Scolymia Spp" ~ "SCOL",
    taxon %in% c("SIDSP", "Sid SP", "SID SP.", "SID SP") ~ "SIDE",
    TRUE ~ taxon
  ))
sort(unique(dca17$taxon))

# Filter out unidentified corals
dca17 <- dca17 %>%
  filter(!taxon %in% c("CORAL", "Cup Coral"))

# Write long data to file
write_csv(dca17, file = "data/processed/dca_2017_long.csv")


# Convert to count data
# Add explicit zeros for any taxon/size class missing at any site
dca17_counts <- dca17 %>%
  mutate(class = ifelse(max_width_cm >= 4, ">4cm", "<4cm")) %>%
  count(site, taxon, class) %>%
  complete(site, taxon, class = c(">4cm", "<4cm"), fill = list(n = 0)) %>%
  # # Don't create zeros for MEAN/MUSS/FAVI adults, since these IDs only applied to juv
  filter(!(taxon %in% c("MEAN", "MUSS", "FAVI") & class == ">4cm" & n == 0))

write_csv(dca17_counts, file = "data/processed/dca_2017_counts.csv")

# Aggregate count data based on taxonomic groups defined above
dca17_counts_ag <- dca17_counts %>%
  left_join(taxon_lookup, by = "taxon") %>%
  mutate(taxon = coalesce(taxon_group, taxon)) %>%
  select(-taxon_group) %>%
  group_by(across(-n)) %>%
  summarize(n = sum(n), .groups = "drop")



# Further aggregate juvenile counts to family (following DRM methods)
dca17_counts_ag <- dca17_counts_ag %>%
  left_join(taxon_lookup_juv, by = "taxon") %>%
  mutate(
    taxon = if_else(class == "<4cm" & !is.na(taxon_group), taxon_group, taxon)
  ) %>%
  select(-taxon_group) %>%
  group_by(across(-n)) %>%
  summarize(n = sum(n), .groups = "drop")


dca17_counts_ag %>%
  distinct(taxon, class) %>%
  arrange(taxon, class)

write_csv(dca17_counts_ag, file = "data/processed/dca_2017_counts_ag.csv")
```

## 2021 TT Recon & ESA Survey

2021 TT site metadata
```{r tt21_site_metadata}
# site metadata
tt21_sitemd0 <- read_csv("data/2021_tt_recon_esa/midpoints_latlon.csv") %>%
  janitor::clean_names()

tt21_sitemd <- tt21_sitemd0 %>%
  mutate(name = str_remove(name, "A$")) %>%
  select(site = name, longitude = lon, latitude = lat)

# There was a lot of sand in these transects that was quantified in final RECON report (though these were the same transects for the ESA and RECON datasets). Extracted this data from RECON report, Table 2:
tt21_sand <- read_csv("data/2021_tt_recon_esa/Table_2_Port_Everglades_RECON.csv") %>%
  janitor::clean_names()

tt21_m_nosand <- tt21_sand %>%
  mutate(area_m2 = 30 - meters_of_sc_sp) %>%
  mutate(site = as.character(site))

# Exclude sites that were 100% sand
allsand <- tt21_m_nosand %>% filter(percent_cover_sc_sp == "100%")

tt21_sitemd <- tt21_sitemd %>% filter(!site %in% allsand$site)
```

2021 TT coral data
```{r tt21_coral_data}
# coral data - recon belt transects
tt21recon0 <- readxl::read_xlsx("data/2021_tt_recon_esa/Recon 30x1m Coral Belt Transect.xlsx") %>%
  janitor::clean_names()

tt21recon <- tt21recon0 %>%
  select(site, taxon = id_abbrev, coral_length_cm, coral_width_cm) %>%
  mutate(taxon = toupper(taxon), site = factor(site)) %>%
  mutate(across(c(coral_length_cm, coral_width_cm), as.numeric)) %>%
  mutate(max_width_cm = pmax(coral_length_cm, coral_width_cm)) %>%
  select(site, taxon, max_width_cm)

# ESA survey data
tt21esa0 <- readxl::read_xlsx("data/2021_tt_recon_esa/ESA Coral Belt Transect.xlsx") %>%
  janitor::clean_names()

sort(unique(tt21esa0$esa_id))

tt21esa <- tt21esa0 %>%
  mutate(site = factor(site),
         taxon = case_when(
           esa_id == "Orbicella franksi" ~ "OFRA",
           esa_id == "Orbicella faveolata" ~ "OFAV",
           esa_id == "Acropora cervicornis" ~ "ACER")) %>%
  filter(!is.na(taxon)) %>%
  mutate(max_width_cm = pmax(coral_length_cm, coral_width_cm)) %>%
  select(site, taxon, max_width_cm)

# Combine Recon and ESA survey data
tt21 <- bind_rows(tt21recon, tt21esa)


# Check taxa names
sort(unique(tt21recon$taxon))

# Filter out unidentified OR NON-CORAL taxa
tt21 <- tt21 %>%
  filter(!taxon %in% c("0", "JUVENILE-UNIDENTIFIABLE", "XESTO", "MALC"))

# Adjust/corrects species IDs
tt21 <- tt21 %>%
  mutate(taxon = case_when(
    taxon == "AFRAG" ~ "AFRA",
    taxon == "FFRAG" ~ "FFRA",
    taxon == "MYALI" ~ "MALI",
    taxon == "MYFER" ~ "MFER",
    taxon == "MYLAM" ~ "MLAM",
    taxon == "ODIF/OROB" ~ "OCUL",
    taxon == "PDCLIV" ~ "PCLI",
    taxon == "PDSTR" ~ "PSTR",
    taxon == "PHYLLANGIA AMERICANA" ~ "PAME",
    taxon == "SCOLYMIA CUBENSIS" ~ "SCUB",
    taxon == "SCOLYMIA LACERA" ~ "SLAC",
    TRUE ~ taxon
  ))
sort(unique(tt21$taxon))

# Write long data to file
write_csv(tt21, file = "data/processed/tt_2021_long.csv")





# Count
# Add explicit zeros for any taxon/size class missing at any site
tt21_counts <- tt21 %>%
  mutate(class = ifelse(max_width_cm >= 4, ">4cm", "<4cm")) %>%
  count(site, taxon, class) %>%
  complete(site, taxon, class = c(">4cm", "<4cm"), fill = list(n = 0)) %>%
  # Don't create zeros for MEAN/MUSS/FAVI adults, since these IDs only applied to juv
  filter(!(taxon %in% c("MEAN", "MUSS", "FAVI") & class == ">4cm" & n == 0))

write_csv(tt21_counts, file = "data/processed/tt_2021_counts.csv")




# Aggregate count data based on taxonomic groups defined above
tt21_counts_ag <- tt21_counts %>%
  left_join(taxon_lookup, by = "taxon") %>%
  mutate(taxon = coalesce(taxon_group, taxon)) %>%
  select(-taxon_group) %>%
  group_by(across(-n)) %>%
  summarize(n = sum(n), .groups = "drop")

# Further aggregate juvenile counts to family (following DRM methods)
tt21_counts_ag <- tt21_counts_ag %>%
  left_join(taxon_lookup_juv, by = "taxon") %>%
  mutate(
    taxon = if_else(class == "<4cm" & !is.na(taxon_group), taxon_group, taxon)
  ) %>%
  select(-taxon_group) %>%
  group_by(across(-n)) %>%
  summarize(n = sum(n), .groups = "drop")

# Exclude data from sites that were recorded as 100% sand
tt21_counts_ag <- tt21_counts_ag %>% filter(site %in% tt21_sitemd$site)

write_csv(tt21_counts_ag, file = "data/processed/tt21_counts_ag.csv")
```

## 2023 TT Impact Survey

TT23 site metadata
```{r 2023_tt_site_metadata}
# Site metadata
tt23_sitemd <- readxl::read_xlsx("data/2023_tt_impact/Impact site tracking.xlsx", skip = 1) %>%
  janitor::clean_names()

tt23_sitemd <- tt23_sitemd %>%
  mutate(site = tolower(transect_name),
         latitude = as.numeric(actual_start_y),
         longitude = as.numeric(actual_start_x)) %>%
  select(site, latitude, longitude) 

# Many sites missing coords in sheet.... whats up with that
tt23_sitemd <- drop_na(tt23_sitemd, longitude)


# missing ten impact site coords



# vertical wall transects
tt23v_sitemd <- readxl::read_xlsx("data/2023_tt_impact/Impact site tracking.xlsx", sheet = "Vertical Wall") %>%
  janitor::clean_names()

tt23v_sitemd <- tt23v_sitemd %>% 
  mutate(
    longitude = (actual_start_x + actual_end_x) / 2,
    latitude = (actual_start_y + actual_end_y) / 2
  ) %>%
  mutate(site = tolower(str_remove_all(name, "[[:space:][:punct:]]"))) %>%
  select(site, latitude, longitude) %>%
  drop_na(site)

# change "cpss" transect names to "cpsse" because this is how they are coded in the coral data (etc.)
tt23v_sitemd <- tt23v_sitemd %>%
  mutate(site = gsub("cpss", "cpsse", site),
         site = gsub("lrnw", "lrmnw", site),
         site = gsub("lrsw", "lrmsw", site))

tt23_sitemd <- bind_rows(
  tt23_sitemd,
  tt23v_sitemd
)
```

TT23 coral data
```{r 2023_tt_coral_data}
tt230 <- readxl::read_xlsx("data/2023_tt_impact/Impact Raw Data 05 31 2024.xlsx") %>%
  janitor::clean_names() 

tt23 <- tt230 %>%
  mutate(transect_name = tolower(str_remove_all(transect_name, "[[:space:][:punct:]]"))) %>%
  select(site = transect_name, depth_ft_start,
         taxon = id_abbrev, coral_length_cm, coral_width_cm) %>%
  filter(taxon != "Xesto") %>%
  mutate(taxon = toupper(taxon)) %>%
  mutate(across(c(coral_length_cm, coral_width_cm), as.numeric)) %>%
  mutate(site = factor(site, levels = sort(unique(site))))

tt23 <- tt23 %>%
  mutate(max_width_cm = pmax(coral_length_cm, coral_width_cm)) %>%
  select(site, taxon, max_width_cm)

tt23 %>%
  filter(taxon == "SLAC") %>%
  print(n = nrow(.))

# Filter to only impact survey and vertical sites (exclude tire reef surveys etc)
sort(unique(tt23$site))
sort(unique(tt23_sitemd$site))
tt23 <- tt23 %>% filter(site %in% tt23_sitemd$site) %>% 
  mutate(site = factor(site, levels = unique(tt23_sitemd$site)))

# Check taxa names
sort(unique(tt23$taxon))

# Filter out unidentified taxa
tt23 <- tt23 %>%
  filter(!taxon %in% c("?", "ID-ABBREV", "NONE", "MHEARD"))

# Adjust/corrects species IDs
tt23 <- tt23 %>%
  mutate(taxon = case_when(
    taxon == "AFRAG" ~ "AFRA",
    taxon %in% c("ASP", "ASP.") ~ "AGAR",
    taxon == "MCAV?" ~ "MCAV",
    taxon == "MSP." ~ "MADR",
    taxon == "MUSSID" ~ "MUSS",
    taxon == "MYALI" ~ "MALI",
    taxon == "MYFER" ~ "MFER",
    taxon == "MYLAM" ~ "MLAM",
    taxon == "OFR" ~ "OFRA",
    taxon == "PCLI?" ~ "PCLI",
    taxon %in% c("PSP", "PSP.") ~ "PORI",
    taxon == "SSP." ~ "SIDE",
    taxon == "STOK" ~ "DSTO",
    TRUE ~ taxon
  ))
sort(unique(tt23$taxon))
tt23 %>% filter(taxon == "MFER")

# Filter out one coral with unmeasured size
tt23 %>% filter(is.na(max_width_cm))
tt23 <- tt23 %>% filter(!is.na(max_width_cm))

# Write long data to file
write_csv(tt23, file = "data/processed/tt_2024_long.csv")





# Count
# Add explicit zeros for any taxon/size class missing at any site
tt23_counts <- tt23 %>%
  mutate(class = ifelse(max_width_cm >= 4, ">4cm", "<4cm")) %>%
  count(site, taxon, class) %>%
  complete(site, taxon, class = c(">4cm", "<4cm"), fill = list(n = 0)) %>%
  # Don't create zeros for MEAN/MUSS/FAVI adults, since these IDs only applied to juv
  filter(!(taxon %in% c("MEAN", "MUSS", "FAVI") & class == ">4cm" & n == 0))

# tt23_counts %>% filter(site == "sb1")
# tt23_sitemd %>% filter(site == "sb1")
write_csv(tt23_counts, file = "data/processed/tt_2024_counts.csv")

tt23_counts %>% filter(site == "cpsn4") %>% print(n=nrow(.))


# Aggregate count data based on taxonomic groups defined above
tt23_counts_ag <- tt23_counts %>%
  left_join(taxon_lookup, by = "taxon") %>%
  mutate(taxon = coalesce(taxon_group, taxon)) %>%
  select(-taxon_group) %>%
  group_by(across(-n)) %>%
  summarize(n = sum(n), .groups = "drop")

tt23_counts_ag %>% filter(site == "cpsn4") %>% print(n=nrow(.))

# Further aggregate juvenile counts to family (following DRM methods)
tt23_counts_ag2 <- tt23_counts_ag %>%
  left_join(taxon_lookup_juv, by = "taxon") %>%
  mutate(
    taxon = if_else(class == "<4cm" & !is.na(taxon_group), taxon_group, taxon)
  ) %>%
  select(-taxon_group) %>%
  group_by(site, taxon, class) %>%
  summarize(n = sum(n), .groups = "drop")

tt23_counts_ag2 %>% filter(site == "cpsn4") %>% print(n=nrow(.))

write_csv(tt23_counts_ag2, file = "data/processed/tt23_counts_ag.csv")


# n_distinct(tt23_counts_ag$site)
# tt23_counts_ag %>%
#   group_by(site) %>%
#   summarize(total = sum(n)) %>%
#   filter(total == 0)


# BUT CPSN4 HAS SLAC? WHERE IS IT?
```


## 2024 DRM/Shedd Survey

Shedd site metadata
```{r 2024_shedd_site_metadata}
# Site metadata
drm24_sitemd <- readxl::read_xlsx("data/2024_shedd_drm/site_metadata.xlsx") %>%
  janitor::clean_names() %>%
  mutate(site = as.character(drm_site_id)) %>%
  select(site, latitude = lat, longitude = lon) %>%
  mutate(depth = NA)
```

Shedd coral data
```{r 2024_shedd_coral_data}
# Adult coral data from main DRM surveys
alldrm2024 <- readxl::read_xlsx("data/2024_shedd_drm/2024ANU_RawCoralDataTransect1and2_Shedd.xlsx") %>%
  janitor::clean_names() %>%
  distinct(site, team, date, subregion)

shedddrm2024 <- alldrm2024 %>% filter(team == "Shedd Aquarium")

# shedddrm2024 %>%
#   count(subregion, date)

# Most sites were included in main DRM database for 2024 -- Import these
adultst1t2 <- readxl::read_xlsx("data/2024_shedd_drm/2024ANU_RawCoralDataTransect1and2_Shedd.xlsx") %>%
  janitor::clean_names() %>%
  filter(subregion == "Broward-Miami", team == "Shedd Aquarium") %>%
  select(site, transect_num, species, width, height)

adultst3t4 <- readxl::read_xlsx("data/2024_shedd_drm/2024ANU_RawCoralDataTransect3and4_Shedd.xlsx") %>%
  janitor::clean_names() %>%
  filter(subregion == "Broward-Miami", team == "Shedd Aquarium") %>%
  select(site, transect_num, species, width, height)

# 9 of our PEV sites were removed from DRM database to avoid oversaturating the ares -- Import these separately
removedt1t2 <- readxl::read_xlsx(
  "data/2024_shedd_drm/2024_DRM_Broward_RemovedSites_T1-T4_Shedd.xlsx", sheet = "Removed Sites T1-T2") %>%
  janitor::clean_names() %>%
  select(site, transect_num, species, width, height)
removedt3t4 <- readxl::read_xlsx(
  "data/2024_shedd_drm/2024_DRM_Broward_RemovedSites_T1-T4_Shedd.xlsx", sheet = "Removed Sites T3-T4") %>%
  janitor::clean_names() %>%
  select(site, transect_num, species, width, height)

# Combine all adult coral data for Shedd DRM surveys at PEV
adults0 <- bind_rows(adultst1t2, adultst3t4, removedt1t2, removedt3t4)
# Convert adult data to long format
adults_long <- adults0 %>%
  mutate(max_width_cm = pmax(width, height, na.rm = TRUE)) %>%
  mutate(max_width_cm = as.character(max_width_cm)) %>%
  mutate(site = str_remove(site, "^AA")) %>%
  select(site, transect_num, taxon = species, max_width_cm) %>%
  drop_na(taxon)     # DROPS when taxon is blank, this is when no corals >4cm were observed

# Import juvenile counts from main DRM dataset
juv <- readxl::read_xlsx("data/2024_shedd_drm/2024ANU_JuvenileCoralData_Shedd.xlsx") %>%
  janitor::clean_names() %>%
  filter(subregion == "Broward-Miami", team == "Shedd Aquarium")

# Import juvenile counts from sites that were removed from main DRM dataset
removed_juv <- readxl::read_xlsx("data/2024_shedd_drm/Shedd_removed_sites_Juveniles_2024.xlsx") %>%
  janitor::clean_names() %>%
  # Missing values in count data should be zero counts (unique to this datasheet from FWC)
  mutate(across(ends_with("_ct"), ~replace_na(., 0)))

# Combine juvenile data
juv0 <- bind_rows(juv, removed_juv) %>%
  mutate(site = str_remove(site, "^AA")) %>%
  select(site, transect_num, ends_with("ct")) %>%
  rename(MCAV = montastraea_ct, MUSS = mussinae_ct, FAVI = faviinae_ct, MEAN = meandrinidae_ct)

# Convert juvenile data to long format
juv_long <- juv0 %>%
  pivot_longer(c(MUSS, FAVI, MEAN, MCAV), names_to = "taxon", values_to = "n") %>%
  mutate(max_width_cm = "<4") %>%
  uncount(n)



# Other juvenile taxa counts from Transects 1 and 2 (DRM 'bonus data')
t1t2bonus <- read_csv("data/2024_shedd_drm/T1_T2_bonus_data.csv") %>%
  janitor::clean_names() %>%
  mutate(site = replace_na(site, "NA")) %>%    # Because one site is called "NA"
  mutate(transect_num = parse_number(transect))
t1t2juv <- t1t2bonus %>%
  select(site, transect_num, starts_with("small")) %>%
  rename_with(~ toupper(gsub("^small_", "", .x)), starts_with("small_"))
t1t2juv_long <- t1t2juv %>%
  pivot_longer(3:10, names_to = "taxon", values_to = "n") %>%
  mutate(max_width_cm = "<4") %>%
  uncount(n)
# Replace site names in t1t2 bonus data with the correct DRM site ID
penipsites <- readxl::read_xlsx("data/2024_shedd_drm/site_metadata.xlsx") %>%
  janitor::clean_names()
t1t2juv_long_updated <- t1t2juv_long %>%
  left_join(penipsites %>% select(site, drm_site_id), by = "site") %>%
  mutate(site = as.character(drm_site_id)) %>%
  select(-drm_site_id)


# Combine all data
drm24_long <- bind_rows(adults_long, juv_long, t1t2juv_long_updated) %>%
  mutate(team = "Shedd Aquarium")

# Check species names
sort(unique(drm24_long$taxon))

write_csv(drm24_long, file = "data/processed/drm_2024_long.csv")


# COUNT based on rules
# ✅ Updated Rules Summary for counting from DRM/Shedd data:
# Juvenile taxa (searched for in <4cm size class only):
#    "MEAN", "MUSS", "FAVI"
#    → these should only ever appear in <4cm, never >4cm, and should not be zero-filled for adults.
# Other juvenile-capable taxa:
#    "MCAV", "SSID", "SRAD", "PAST", "PPOR", "SINT", "SBOU", "AAGA", "MAUR"
#    → these can be counted in both >4cm and <4cm, but only in <4cm if juveniles were searched on that transect and team.
# Transect-based search rules still apply:
# Transects 1 & 2: all adult taxa always searched. Juvenile search depends on team:
#    "Shedd Aquarium" → all juvenile taxa above searched
#    others → only MEAN, MUSS, FAVI, MCAV
# Transects 3 & 4:
#    only subset of adult taxa searched (adult_taxa_t3t4)
#    only juveniles: MEAN, MUSS, FAVI, MCAV

# Step 1: Define size classes
drm24_classed <- drm24_long %>%
  mutate(class = case_when(as.numeric(max_width_cm) >= 4 ~ ">4cm",
                           max_width_cm == "<4" ~ "<4cm"))

# Step 2: Define species sets
all_taxa <- unique(drm24_classed$taxon)
adult_taxa_t3t4 <- c("CNAT", "DSTO", "DLAB", "MMEA", "MANG", "MALI", 
                     "MFER", "MLAM", "PCLI", "PSTR")
juv_only_taxa <- c("MEAN", "MUSS", "FAVI")
juv_both_taxa <- c("MCAV", "SSID", "SRAD", "PAST", "PPOR", "SINT", "SBOU", "AAGA", "MAUR")
all_juv_taxa <- c(juv_only_taxa, juv_both_taxa)

# Step 3: Build search grid per site × transect × team
search_grid <- drm24_classed %>%
  distinct(site, team) %>%    # if multiple teams in data, remove value for team
  expand_grid(transect_num = 1:4) %>%  # creates search grid for all transects even if no corals observed (bc absent from drm24_classed)
  mutate(
    searched_taxa_class = pmap(list(transect_num, team), function(transect, team) {
      # Helper: define juv taxa allowed for this transect/team
      juv_taxa <- if (transect %in% c(1, 2)) {
        if (team == "Shedd Aquarium") {      # Shedd searched for other juv taxa on T1 and T2, other DRM survey teams did not
          all_juv_taxa
        } else {
          c(juv_only_taxa, "MCAV")
        }
      } else {
        c(juv_only_taxa, "MCAV")
      }
      
      # Adults always searched in 1 & 2, subset in 3 & 4
      adult_taxa <- if (transect %in% c(1, 2)) {
        setdiff(all_taxa, juv_only_taxa)  # exclude juv-only taxa
      } else {
        adult_taxa_t3t4
      }

      # Build grid
      bind_rows(
        expand_grid(taxon = adult_taxa, class = ">4cm"),
        expand_grid(taxon = juv_taxa, class = "<4cm")
      )
    })
  ) %>%
  unnest(searched_taxa_class)

# Step 4: Count observations
counts <- drm24_classed %>%
  group_by(site, transect_num, team, taxon, class) %>%
  summarize(n = n(), .groups = "drop")


# Step 5: Join with grid and fill in zeros where appropriate
final_counts <- search_grid %>%
  left_join(counts, by = c("site", "transect_num", "team", "taxon", "class")) %>%
  mutate(n = replace_na(n, 0))


write_csv(final_counts, file = "data/processed/drm_2024_counts.csv")




# AGGREGATE COUNT DATA
# Aggregate taxa
drm24_counts_ag <- final_counts %>%
  left_join(taxon_lookup, by = "taxon") %>%
  mutate(taxon = coalesce(taxon_group, taxon)) %>%
  select(-taxon_group) %>%
  group_by(across(-n)) %>%
  summarize(n = sum(n), .groups = "drop")

write_csv(drm24_counts_ag, file = "data/processed/drm_2024_counts_ag.csv")


# Modify DRM2024 dataset to code as one pseudo-transect per site, since all other datasets have only one transect per site. We need the structure to be uniform across datasets for count modeling -- cannot include random effect for site to account for multiple transects per site while all other datasets only have one transect per site, because then there's no variability associated with site and its essentially treated as a fixed effect, which we don't want. Thus, we need to aggregate across transects in the DRM 2024 dataset. But since different taxa were searched for on different numbers of transects we need to do this carefully and have the appropriate total area searched for each taxon for teh pseudo-transect.

# Assign uniform transect area
drm24_counts_ag <- drm24_counts_ag %>%
  mutate(transect_area_m2 = 10)

# Aggregate to pseudo-transect per site × taxon × class
drm24_counts_agg <- drm24_counts_ag %>%
  group_by(site, taxon, class) %>%
  summarise(
    n = sum(n, na.rm = TRUE),
    transect_area_m2 = sum(transect_area_m2),
    .groups = "drop"
  )



## Exclude two sites that were selected specifically because of known ACER populations
drm24_counts_agg <- drm24_counts_agg %>%
  filter(!site %in% c("3096", "3094"))

# Update metadata
drm24_sitemd <- drm24_sitemd %>% filter(site %in% unique(drm24_counts_agg$site))
```

## Habitat maps
```{r habitat_maps, fig.width = 10, fig.height = 10}
# --- STEP 1: Load KML Polygons ---
polygons <- st_read("data/Habitat classifications.kml")  # update path as needed

# --- STEP 2: Extract Attributes from HTML Description ---
extract_attrs <- function(desc) {
  if (is.na(desc) || desc == "") {
    return(tibble(Habitat = NA, Type = NA, Modifier = NA, Region = NA, Type2 = NA))
  }
  html <- read_html(desc)
  rows <- xml_find_all(html, "//table//table//tr")
  keys <- rows %>% xml_find_all(".//td[1]") %>% xml_text(trim = TRUE)
  vals <- rows %>% xml_find_all(".//td[2]") %>% xml_text(trim = TRUE)
  n <- min(length(keys), length(vals))
  named_vals <- set_names(vals[1:n], keys[1:n])
  tibble(
    Habitat  = named_vals[["Habitat"]],
    Type     = named_vals[["Type"]],
    Modifier = named_vals[["Modifier"]],
    Region   = named_vals[["Region"]],
    Type2    = named_vals[["Type2"]]
  )
}

# Apply function and combine with spatial geometries
attrs <- map_dfr(polygons$Description, extract_attrs)
polygons_clean <- bind_cols(polygons %>% select(-Description), attrs)


###### GET RID OF OVERLAPPING SAND POLYGONS

# 1. Set clean CRS and define area of interest (AOI)
crs_clean <- st_crs(32617)

aoi_bbox <- st_as_sfc(st_bbox(c(xmin = -80.113, xmax = -80.078, ymin = 26.058, ymax = 26.112), crs = 4326))
aoi_bbox_utm <- st_transform(aoi_bbox, crs = crs_clean)

# 2. Reproject and filter
polygons_proj <- st_transform(polygons_clean, crs = crs_clean)

sand_polygons <- polygons_proj %>%
  filter(Type == "Sand") %>%
  st_intersection(aoi_bbox_utm)

non_sand_polygons <- polygons_proj %>%
  filter(Type != "Sand") %>%
  st_intersection(aoi_bbox_utm)

# 3. Clean geometries using WKT rebuild
sand_geom_clean <- st_sfc(
  lapply(st_as_text(st_geometry(sand_polygons)), function(wkt) st_as_sfc(wkt)[[1]]),
  crs = crs_clean
)

nonsand_union_clean <- st_as_sfc(st_as_text(st_union(st_geometry(non_sand_polygons))), crs = crs_clean)[[1]]

# 4. Subtract all non-sand from sand polygons
cut_geoms <- lapply(seq_along(sand_geom_clean), function(i) {
  g <- sand_geom_clean[[i]]
  g_fixed <- st_buffer(g, 0)
  tryCatch(
    st_difference(g_fixed, nonsand_union_clean),
    error = function(e) {
      message("Skipping geometry ", i, " due to error: ", e$message)
      st_geometrycollection()
    }
  )
})

# 5. Rebuild sand_cut sf object and drop empty geometries
sand_cut <- sand_polygons
st_geometry(sand_cut) <- st_sfc(cut_geoms, crs = crs_clean)
sand_cut <- sand_cut[!st_is_empty(sand_cut), ]

# 6. Combine everything
combined <- bind_rows(sand_cut, non_sand_polygons)

# 7. Reproject to WGS84 for plotting
polygons_final <- st_transform(combined, crs = 4326)

```



## Assign sites to habitat types
```{r}

######## MERGE WITH SURVEY SITES
# --- STEP 3: Prepare Site Coordinate Data ---
# Get all site coordinates, and assign north and south
# Combine site metadata
allsitemd <- bind_rows(.id = "dataset",
  nsu11_esa = nsu11_esa_sitemd,
  dca17_esa = dca17_esa_sitemd,
  dca17 = dca17_sitemd,
  tt21 = tt21_sitemd,
  tt23 = tt23_sitemd,
  drm24 = drm24_sitemd
) %>%
  mutate(dir = if_else(latitude > 26.093570, "N", "S"))
points <- st_as_sf(allsitemd, coords = c("longitude", "latitude"), crs = 4326)

# --- STEP 4: Validate Geometry and Match CRS ---
polygons_final <- polygons_final %>%
  st_zm(drop = TRUE, what = "ZM") %>%
  st_make_valid() %>%
  st_transform(st_crs(points))

sf_use_s2(FALSE)  # prevent s2 geometry issues

# --- STEP 5: Spatial Join ---
joined <- st_join(points, polygons_final, join = st_within)

joined_df <- joined %>%
  mutate(longitude = st_coordinates(.)[,1],
         latitude = st_coordinates(.)[,2]) %>%
  st_drop_geometry()

allsitemd <- joined_df

# Add factor if survey was ESA coral species only
allsitemd <- allsitemd %>%
  mutate(ESAonly = if_else(dataset %in% c("nsu11_esa", "dca17_esa"), "ESA only", "All corals"))

# Visualize habitat classifications
polyplot <- polygons_final %>% 
  #filter(Type != "Sand") %>%
  ggplot() +
  geom_sf(aes(fill = Type), color = NA, size = 0.1, alpha = 0.5) +
  scale_fill_brewer(palette = "Set3", na.value = "gray80") +
  theme_minimal(base_size = 8) +
  labs(fill = NULL, shape = NULL, x = NULL, y = NULL) +
  theme(legend.position = "right") +
  coord_sf(
    xlim = c(-80.113, -80.078),  # Adjusted to reduce whitespace
    ylim = c(26.058, 26.112),
    expand = FALSE
  ) +
  scale_y_continuous(labels = scales::number_format(accuracy = 0.01)) +
  scale_x_continuous(breaks = seq(-80.11, -80.08, by = 0.01),
                     labels = scales::number_format(accuracy = 0.01))  # Fewer long ticks


# Plot all coral surveys and ESA coral surveys
esa_all_plot <- polyplot +
  geom_point(data = filter(allsitemd, ESAonly == "All corals"),
             aes(x = longitude, y = latitude, shape = dataset),
             inherit.aes = FALSE, alpha = 0.3, size = 0.5) +
  scale_shape_manual(values = c(2, 1, 4, 5)) +
  guides(fill = "none", shape = "none") +
  theme_minimal()
 
esa_esa_plot <- polyplot +
  geom_point(data = filter(allsitemd, ESAonly == "ESA only"),
             aes(x = longitude, y = latitude, shape = dataset),
             inherit.aes = FALSE, alpha = 0.3, size = 0.5) +
  scale_shape_manual(values = c(3, 6)) +
  guides(fill = "none", shape = "none") +
  theme_minimal() #+
  # theme(plot.title = element_text(hjust = 0, face = "bold", size = 9))

# cowplot::plot_grid(esa_all_plot, esa_esa_plot)
```

## Impact Zones
```{r}
# Impact zone Scenario 2 and NMFS consideration areas
# Define the extraction function once
extract_placemark_info <- function(kml_file, source_label) {
  ns <- c(kml = "http://www.opengis.net/kml/2.2")
  kml <- read_xml(kml_file)
  placemarks <- xml_find_all(kml, ".//kml:Placemark", ns)

  map(placemarks, function(pm) {
    name <- xml_text(xml_find_first(pm, ".//kml:name", ns))

    folder_node <- xml_parent(xml_parent(pm))
    folder_name <- xml_text(xml_find_first(folder_node, ".//kml:name", ns))
    if (is.na(folder_name) || folder_name == "") folder_name <- "Top Level"

    coord_nodes <- xml_find_all(pm, ".//kml:coordinates", ns)

    # Extract all coordinate rings
    rings <- lapply(coord_nodes, function(cn) {
      coords_text <- xml_text(cn)
      coords <- strsplit(trimws(coords_text), "\\s+")[[1]]
      coords_mat <- do.call(rbind, lapply(coords, function(x) {
        as.numeric(strsplit(x, ",")[[1]][1:2])
      }))
      if (nrow(coords_mat) >= 3) coords_mat else NULL
    })

    rings <- compact(rings)
    if (length(rings) == 0) return(NULL)

    # Group into separate polygon parts based on breaks (every outer ring followed by 0+ inner rings)
    # For now, assume all are outer rings (no holes), so wrap each as its own polygon
    multipoly <- st_multipolygon(lapply(rings, function(r) list(r)))

    tibble(
      folder = folder_name,
      name = name,
      geometry = st_sfc(multipoly, crs = 4326),
      source = source_label
    )
  }) %>%
    compact() %>%
    bind_rows() %>%
    st_as_sf()
}



# File paths
impact_kml <- "data/Impact_zones_Scenario 2.kml"
nmfs_kml <- "data/Scenario2_NMFSConsiderationAreas.kml"

# Extract and label
impact_zones_sf <- extract_placemark_info(impact_kml, "Impact Zone")
nmfs_sf <- extract_placemark_info(nmfs_kml, "NMFS Consideration Area")

# Combine
all_zones_sf <- bind_rows(impact_zones_sf, nmfs_sf)

# Rename polygons based on zone folder
all_zones_sf <- all_zones_sf %>%
  mutate(
    name = if_else(
      is.na(name) | name == "",
      str_remove(str_remove(folder, "^Scenario_2_"), "\\.shp$"),
      name
    ),
    scenario = if_else(name %in% c("North", "South", "East", "Nearshore"), "NMFS Consideration", "Scenario 2")
  )

# Create new polygon covering Rest of Monitoring Area, from 1.2 km N to 1.2 km S of the channel

# 1. Extract the 'Channel' polygon
channel_poly <- all_zones_sf %>%
  filter(str_detect(tolower(name), "channel"))

# 2. Transform all relevant layers to UTM (meters) for buffering
utm_crs <- 32617  # UTM Zone 17N — suitable for SE Florida

if (st_is_longlat(channel_poly)) {
  channel_poly <- st_transform(channel_poly, utm_crs)
  polygons_clean_m <- st_transform(polygons_clean, utm_crs)
  all_zones_sf_m <- st_transform(all_zones_sf, utm_crs)
} else {
  polygons_clean_m <- polygons_clean
  all_zones_sf_m <- all_zones_sf
}

# 3. Get bounds of channel (north-south)
bbox_channel <- st_bbox(channel_poly)

# 4. Define rectangular box: full E-W extent of reef habitat, ±1.2 km N/S of channel
ew_range <- st_bbox(polygons_clean_m)[c("xmin", "xmax")]
ns_range <- c(bbox_channel["ymin"] - 1200, bbox_channel["ymax"] + 1200)
names(ns_range) <- c("ymin", "ymax")

new_box <- st_as_sfc(st_bbox(c(
  xmin = ew_range[[1]],
  xmax = ew_range[[2]],
  ymin = ns_range[[1]],
  ymax = ns_range[[2]]
), crs = st_crs(polygons_clean_m)))

# 5. Clip the box to reef habitat
intersected_box <- st_union(st_intersection(new_box, st_union(polygons_clean_m)))

# 6. Subtract all existing zones (Impact + NMFS) by taking concave hull
# 6. Create a tight, shrink-wrapped outer boundary around all existing zones
existing_zone_hull <- all_zones_sf_m %>%
  st_make_valid() %>% 
  st_union() %>%
  st_buffer(20) %>%     # Buffer OUTWARD 40 meters to close gaps (adjust as needed)
  st_union() %>%
  st_buffer(-20)        # Buffer INWARD to return to approximate original size

# 7. Subtract existing zones from monitoring area
new_zone_geom <- st_difference(intersected_box, existing_zone_hull)

# 8. Build the new polygon row
new_zone <- st_sf(
  folder = "Generated",
  name = "Rest of Monitoring Area",
  scenario = "Rest of Monitoring Area",
  geometry = st_sfc(new_zone_geom),
  source = "Impact Zone",
  crs = st_crs(polygons_clean_m)
)

# 9. Transform back to match original CRS (likely WGS84)
if (st_crs(new_zone) != st_crs(all_zones_sf)) {
  new_zone <- st_transform(new_zone, st_crs(all_zones_sf))
}

# 10. Add the new zone to the combined zones object
all_zones_sf <- bind_rows(all_zones_sf, new_zone)
zone_levels <- c("Channel", "Side Slopes", "Depth_10cm", "Depth_5cm", "Depth_1cm", "Depth_0_5cm", "Depth_0_1cm",
                 "Nearshore", "North", "East", "South", "Rest of Monitoring Area")
all_zones_sf$name <- factor(all_zones_sf$name, levels = zone_levels)



#### MERGE CONTIGUOUS POLYGONS OF SAME ZONE TYPE/NAME


# 1. Ensure individual polygons (not MULTIPOLYGONs)
zones_split <- all_zones_sf %>%
  st_cast("POLYGON") %>%
  mutate(row_id = row_number())  # unique ID to track rows

# 2. Group and merge contiguous polygons with the same name
zones_merged <- zones_split %>%
  group_by(name) %>%
  group_split() %>%
  map_dfr(function(df) {
    # Build adjacency graph for touching polygons
    touch_mat <- st_touches(df)
    g <- graph_from_adj_list(touch_mat)

    # Find connected components
    comps <- components(g)$membership

    # Merge by component
    df %>%
      mutate(component = comps) %>%
      group_by(name, component) %>%
      summarise(
        geometry = st_union(geometry),
        scenario = first(scenario),
        source = first(source),
        .groups = "drop"
      )
  }) %>%
  select(name, scenario, source, geometry) %>%
  st_as_sf()


#### PLOT




########### gradient colors
# 1. Transform to UTM Zone 17N for proper distance in meters
utm_crs <- 32617
zones_utm <- st_transform(zones_merged, utm_crs)
channel_utm <- st_transform(channel_poly, utm_crs)

# 2. Get Y coordinate of channel centroid
channel_centroid_y <- st_coordinates(st_centroid(channel_utm))[,"Y"]

# 3. Compute distance from channel (north/south) in meters
zones_with_dist <- zones_utm %>%
  mutate(
    is_channel = name == "Channel",
    centroid = st_centroid(geometry),
    centroid_y = st_coordinates(centroid)[,"Y"],
    dist_from_channel = if_else(
      is_channel,
      0,
      abs(centroid_y - channel_centroid_y)
    )
  )

# Define a more distinct gradient palette
distance_palette <- c(
  "#B2182B",  # deep red
  "#EF8A62",  # reddish-orange
  "#FDB863",  # orange
  "#FFFFBF"  # yellow
)

# Plot
p <- ggplot() +
  # Habitat polygons (no legend)
  geom_sf(data = polygons_clean, aes(fill = Type),
          color = NA, size = 0.1, alpha = 0.25, show.legend = FALSE) +
  scale_fill_brewer(palette = "Set3", na.value = "gray80") +

  ggnewscale::new_scale_fill() +

  # Zones (with 12 manual colors and legend)
  geom_sf(data = zones_with_dist %>% filter(!is_channel),
          aes(fill = dist_from_channel), color = "black", alpha = 0.5, linewidth = 0.075) +
  geom_sf(data = zones_with_dist %>% filter(is_channel),
          fill = "#B2182B", color = "black", linewidth = 0.075, alpha = 0.5) +  # Red for Channel
  scale_fill_gradientn(
    colors = distance_palette,
    name = "Distance from Channel (m)"
  ) +

  theme_minimal() +
  theme(legend.position = "none") +
  coord_sf(
    xlim = c(-80.113, -80.078),
    ylim = c(26.075, 26.11),
    expand = FALSE, clip = "off"
  ) +
  scale_y_continuous(labels = scales::number_format(accuracy = 0.001)) +
  scale_x_continuous(breaks = seq(-80.11, -80.08, by = 0.01),
                     labels = scales::number_format(accuracy = 0.01))

# LABEL ZONES
zone_labels <- tribble(
  ~name,                   ~label_x, ~label_y,
  "Side Slopes",           -80.104,  26.08,
  "Depth_10cm",            -80.103,  26.078,
  "Depth_5cm",             -80.102,  26.076,
  "Depth_1cm",             -80.101,  26.074,
  "Depth_0_5cm",           -80.100,  26.072,
  "Depth_0_1cm",           -80.099,  26.070,
  "Nearshore",             -80.106,  26.094,
  "North",                 -80.0795,  26.097,
  "East",                  -80.080, 26.094,
  "South",                 -80.0793,  26.091,
  "Impact Monitoring Area",       -80.0815, 26.083
)

zone_segments <- tribble(
  ~name,                   ~segment_x, ~segment_y,
  "Side Slopes",           -80.102,    26.0945,
  "Side Slopes",           -80.1025,    26.093,
  "Depth_10cm",            -80.101,   26.0947,
  "Depth_10cm",            -80.1015,   26.09285,
  "Depth_5cm",             -80.100,    26.0949,
  "Depth_5cm",             -80.1005,    26.0926,
  "Depth_1cm",             -80.0985,   26.0957,
  "Depth_1cm",             -80.0992,   26.092,
  "Depth_0_5cm",           -80.097,   26.0968,
  "Depth_0_5cm",           -80.098,   26.091,
  "Depth_0_1cm",           -80.0954,   26.098,
  "Depth_0_1cm",           -80.097,   26.0897,
  "Nearshore",             -80.106,    26.09,
  "North",                 -80.0795,  26.097,
  "East",                  -80.080, 26.094,
  "South",                 -80.0793,  26.091,
  "Impact Monitoring Area",       -80.0815, 26.083
)

label_segments <- zone_segments %>%
  left_join(zone_labels, by = "name")

iz_plot <- p +
  geom_segment(data = label_segments,
               aes(x = segment_x, y = segment_y,
                   xend = label_x, yend = label_y),
               color = "black", linewidth = 0.2) +
  geom_text(data = zone_labels,
            aes(x = label_x, y = label_y, label = name),
            size = 2, hjust = 1) +
  coord_sf(
    xlim = c(-80.113, -80.078),
    ylim = c(26.058, 26.112),
    expand = FALSE
  ) +
  labs(x = "", y = "") +
  scale_y_continuous(labels = scales::number_format(accuracy = 0.01)) +
  scale_x_continuous(breaks = seq(-80.11, -80.08, by = 0.01),
                     labels = scales::number_format(accuracy = 0.01))

iz_plot
```

## Multipanel map figure
```{r}
# Shared axis limits for all maps
coord_limits <- coord_sf(
  xlim = c(-80.113, -80.078),
  ylim = c(26.058, 26.112),
  expand = FALSE
)

# Apply consistent theme + coordinate limits
base_theme <- theme_minimal(base_size = 7) +
  theme(
    axis.title = element_blank(),
    axis.text.y = element_text(size = 6),
    axis.text.x = element_text(size = 6),
    legend.position = "none",
    plot.margin = margin(5,0,0,0, unit = "mm"),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA),
    legend.spacing.y = unit(0.05, "cm"),    # reduce vertical space between keys
    legend.key.height = unit(0.2, "cm"),   # reduce height of each legend key box
    legend.key.width = unit(0.4, "cm"),
    legend.text = element_text(margin = margin(0,0,0,0)),  # tighten text spacing
    legend.title = element_text(margin = margin(0,0,0,0)),
    legend.margin = margin(1,1,1,0),
    legend.title.align = 0.5,
    legend.key = element_blank(),
    legend.background = element_blank(),
    legend.box.background = element_rect(color = NA, fill = alpha("white", 0.7))
  )

# 1. Left panel: All corals (keep y-axis)
esa_all_plot_clean <- esa_all_plot +
  coord_limits +
  base_theme +
  guides(shape = "legend") +
  theme(legend.position = c(0.75, 0.2))

# 2. Middle panel: ESA corals (remove y-axis)
esa_esa_plot_clean <- esa_esa_plot +
  coord_limits +
  base_theme +
  guides(shape = "legend") +
  theme(axis.text.y = element_blank(),
        legend.position = c(0.25, 0.2))

# 3. Right panel: Impact zones (remove y-axis)
iz_plot_clean <- iz_plot +
  coord_limits +
  base_theme +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

# 4. Extract compact fill legend from polyplot
legend_plot <- polyplot +
  guides(shape = "none", color = "none", fill = guide_legend(nrow = 2)) +
  base_theme +
  theme(
    legend.position = "bottom",
    legend.key.size = unit(0.3, "cm"),  # Reduce square size
    legend.text = element_text(size = 6),  # Optional: smaller legend text
    legend.title = element_text(size = 7)  # Optional: smaller legend title
  )
grob <- ggplotGrob(legend_plot)
fill_legend <- gtable::gtable_filter(grob, "guide-box")

# 5. Align the plot panels (so heights match)
aligned_maps <- cowplot::align_plots(
  esa_all_plot_clean, esa_esa_plot_clean, iz_plot_clean,
  align = "hv", axis = "tblr"
)

# 6. Assemble horizontal row of aligned plots
map_row <- cowplot::plot_grid(
  plotlist = aligned_maps,
  labels = c("A. Surveys of all corals", "B. Surveys of ESA corals", "C. Impact zones"),
  label_size = 8,
  label_fontface = "bold",
  label_x = c(-0.2, -0.2, -0.1),
  label_y = 1,
  nrow = 1,
  rel_widths = c(1, 1, 1)
)

# 7. Add the legend below the row
final_plot <- cowplot::plot_grid(
  map_row,
  fill_legend,
  ncol = 1,
  rel_heights = c(1, 0.07)
)

# 8. Draw final plot
ggdraw() + draw_plot(final_plot) +
  theme(plot.background = element_rect(fill = "white", color = NA))


ggsave(filename = "outputs/Fig1.png", width = 170, height = 100, units = "mm", dpi = 300)
```



## Area of each habitat type in each impact zone
```{r}
# Make sure 'Artificial' is changed to 'Nearshore Ridge Complex' in habitat polygons, and that Aggregated Patch Reef is changed to Outer Reef
polygons_filtered <- polygons_final %>%
  mutate(Type = if_else(Type == "Artificial", "Nearshore Ridge Complex", Type)) %>%
  mutate(Type = if_else(Type == "Aggregated Patch Reef", "Outer Reef", Type)) %>%
  filter(Type %in% type_levels)

# Clean any invalid geometry
polygons_filtered <- st_make_valid(polygons_filtered)
all_zones_sf <- st_make_valid(all_zones_sf)
# Intersect with habitat polygons
habitat_in_zones <- st_intersection(polygons_filtered, all_zones_sf)

# Project for accurate area
habitat_in_zones_proj <- habitat_in_zones %>%
  st_transform(32617) %>%
  mutate(area_m2 = st_area(geometry))

# Summarize area by habitat type and zone name
area_summary <- habitat_in_zones_proj %>%
  st_drop_geometry() %>%
  mutate(name = factor(name, levels = zone_levels),
         Type = factor(Type, levels = type_levels)) %>%
  group_by(Type, name) %>%
  summarize(total_area_m2 = sum(as.numeric(area_m2)), .groups = "drop") %>%
  arrange(name)

# Print total area of each habitat in each impact zone
area_summary %>% 
  pivot_wider(names_from = name, values_from = total_area_m2, names_sort = FALSE)

# Combine for full Scenario2+NMFS, and Rest of Monitoring Area
area_summary2 <- habitat_in_zones_proj %>%
  st_drop_geometry() %>%
  mutate(Type = factor(Type, levels = type_levels)) %>%
  group_by(Type, scenario) %>%
  summarize(total_area_m2 = sum(as.numeric(area_m2)), .groups = "drop")

area_summary2 %>% 
  pivot_wider(names_from = scenario, values_from = total_area_m2)

```

# Combine and filter data

## Combine surveys datasets
```{r combine_data}
# Combine all aggregated count data
df <- bind_rows(.id = "dataset",
  nsu11_esa = nsu11_esa_counts_ag,
  dca17_esa = dca17_esa_counts_ag_full,
  dca17 = dca17_counts_ag,
  tt21 = tt21_counts_ag,
  tt23 = tt23_counts_ag2,
  drm24 = drm24_counts_agg     # use data aggregated to 1 pseudo-transect - already has transect_area_m2
)
```

## Filter out rare taxa
```{r}
# Filter out taxa with very few observations
# Drop taxa with very few observations
sppcounts <- df %>%
  group_by(taxon) %>%
  summarize(total = sum(n), .groups = "drop") %>%
  arrange(total) 
sppcounts 

df <- df %>%
  filter(taxon %in% filter(sppcounts, total >= 5)$taxon)
# Taxa with less than 5 total observations were filtered out, which included: FFRA, HCUC, PAME, SCOL, OCUL


# Ensure explicit zeros are included for all taxa searched for but not observed (e.g., ACER never recorded in TT23 dataset)
# Step 1: Get taxon:class combos from the full df (i.e., all combos ever observed anywhere)
taxon_class_combos <- df %>%
  distinct(taxon, class)

# Step 2: Split df into those to complete vs those to leave untouched
df_base <- df %>% filter(dataset %in% c("nsu11_esa", "dca17_esa"))
df_to_complete <- df %>% filter(!dataset %in% c("nsu11_esa", "dca17_esa"))

# Step 3: Get all site × dataset combinations to complete
site_info <- df_to_complete %>%
  distinct(dataset, site)

# Step 4: Create full grid for these datasets only
full_grid <- site_info %>%
  tidyr::crossing(taxon_class_combos)

# Step 5: Join and fill in zeros
df_completed_part <- full_grid %>%
  left_join(df_to_complete, by = c("dataset", "site", "taxon", "class")) %>%
  mutate(n = coalesce(n, 0)) %>%
  select(dataset, site, taxon, class, n)

# Step 6: Combine with the untouched ESA datasets
df_completed <- bind_rows(df_base, df_completed_part)


# Add transect area information
## Bring back in DRM24 dataset taxon:class-specific transect areas searched
df_completed <- df_completed %>% 
  left_join(distinct(drm24_counts_agg, taxon, class, transect_area_m2) %>%
              mutate(dataset = "drm24"), 
            by = c("dataset", "taxon", "class")) %>%
  select(-transect_area_m2.x, transect_area_m2 = transect_area_m2.y)

## ACER and ORBI <4cm didn't get assigned areas because were never observed -- these would have been searched in 20 m2 (T1 and T2)
df_completed %>% filter(is.na(transect_area_m2), dataset == "drm24") %>%
  distinct(taxon, class)
df_completed <- df_completed %>%
  mutate(transect_area_m2 = case_when(
    dataset == "drm24" & is.na(transect_area_m2) ~ 20,
    TRUE ~ transect_area_m2
  ))

## Add transect area information for other datasets
## Site-specific areas for tt21 (since there was sand in transects)
tt21_areas <- tt21_m_nosand %>%
  mutate(dataset = "tt21") %>%
  distinct(dataset, site, transect_area_m2 = area_m2)

df_completed <- df_completed %>%
  left_join(tt21_areas, by = c("dataset", "site")) %>%
  mutate(
    transect_area_m2 = coalesce(transect_area_m2.x, transect_area_m2.y)
  ) %>%
  select(-transect_area_m2.x, -transect_area_m2.y)

## Other datasets had fixed transect areas
df_completed <- df_completed %>%
  mutate(transect_area_m2 = case_when(
     # Count-specific areas for nsu11 (since they did tier 1 surveys and tier 2 only if n > 5 for a taxon)
    dataset == "nsu11_esa" & taxon == "ACER" & n >= 5 ~ 600,  
    dataset == "nsu11_esa" & taxon == "ACER" & n < 5 ~ 3600,
    dataset == "nsu11_esa" & taxon != "ACER" ~ 600,
    dataset == "dca17_esa" ~ 784,
    dataset == "dca17" ~ 30,     # DCA belt transects were 30m
    dataset == "tt23" ~ 20,     # TT23 belt transects were 20m
    TRUE ~ transect_area_m2))


# check that everything has transect_area_m2
df_completed %>% filter(is.na(transect_area_m2))
```




## Filter by habitat

```{r select_habitats, fig.width = 10, fig.height = 10}
allsitemd %>% count(Type)

#### RECLASSIFY VERTICAL TRANSECTS MANUALLY AS A SEPARATE HABITAT TYPE?
# probably yes. 

# Select sites in Nearshore Ridge Complex, Inner Reef, and Middle Reef, Outer Reef and Aggregated Patch Reef
selected <- allsitemd %>%
  filter(Type %in% c("Nearshore Ridge Complex", "Inner Reef", "Middle Reef", 
                     "Artificial", "Outer Reef", "Aggregated Patch Reef"))

# Which sites were excluded?
excluded <- anti_join(allsitemd, selected) 

excluded %>%
  count(dataset, Type)

#df_completed %>% right_join(excluded) %>% arrange(-n) %>% print(n=100)

# Plot all selected sites
polyplot + 
  geom_point(data = selected, 
             aes(x = longitude, y = latitude, shape = dataset), 
             inherit.aes = FALSE, alpha = 0.6) +
  scale_shape_manual(values = c(2, 3, 4, 5, 6, 7)) +
  facet_wrap(~ESAonly) +
  labs(title = "Selected sites - all surveys")


# Filter dataset to just selected sites
dff <- df_completed %>% inner_join(selected)
```


Shedd dataset has some sites in the NRC south that are further from the channel relative to other datasets. Are these similar enough to the NRC south sites that are closer to the channel that they can be reasonably included?

First, look at total coral density between the nearer vs. farther sites

```{r NRCS_density}
drm24_NRCS <- drm24_counts_ag %>%
  left_join(allsitemd) %>%
  filter(Type == "Nearshore Ridge Complex", dir == "S")

drm24_NRCS <- drm24_NRCS %>%
  mutate(grp = if_else(latitude > 26.08, "near", "far"))

# Fit a Negative Binomial GLM
mod_nb <- MASS::glm.nb(n ~ grp, data = drm24_NRCS)

# Generate new data only for existing taxon-size class combinations
newdata_1 <- drm24_NRCS %>%
  distinct(grp)

# Get predicted values & standard errors (log scale)
preds_nb <- predict(mod_nb, newdata_1, type = "link", se.fit = TRUE)

# Compute total coral density 
results_nb <- newdata_1 %>%
  mutate(
    fit = exp(preds_nb$fit),                    # Convert fitted values to response scale
    fit_se = exp(preds_nb$fit) * preds_nb$se.fit,   # Convert SE using the Delta Method
    fit_var = (fit * preds_nb$se.fit)^2,         # Variance propagation
    fit_lower = exp(preds_nb$fit - 1.96 * preds_nb$se.fit),  # Lower CI
    fit_upper = exp(preds_nb$fit + 1.96 * preds_nb$se.fit)   # Upper CI
  )

# Compute total coral density + confidence intervals
total_ci_nb <- results_nb %>%
  group_by(grp) %>%
  summarize(
    total_density = sum(fit),
    total_se = sqrt(sum(fit_var)),
    lower_95CI = exp(log(total_density) - 1.96 * (total_se / total_density)),
    upper_95CI = exp(log(total_density) + 1.96 * (total_se / total_density))
  )

knitr::kable(total_ci_nb)

```

Next, look at community composition between the nearer vs. farther sites
```{r NRCS_community_composition}

# 1. Create a wide community matrix: rows = site x grp, columns = taxa
comm_matrix <- drm24_NRCS %>%
  group_by(site, grp, taxon) %>%
  summarize(total_n = sum(n), .groups = "drop") %>%
  pivot_wider(names_from = taxon, values_from = total_n, values_fill = 0)

# 2. Extract community matrix and metadata
comm_data <- comm_matrix %>% select(-site, -grp)
site_info <- comm_matrix %>% select(site, grp)

# 3. Run NMDS (k = 2 dimensions is standard)
set.seed(123)  # for reproducibility
nmds <- metaMDS(comm_data, distance = "bray", k = 2, trymax = 100)

# 4. Prepare data for plotting
scores_df <- scores(nmds)$sites %>%
  bind_cols(site_info)

# 5. Plot NMDS with ggplot2
ggplot(scores_df, aes(x = NMDS1, y = NMDS2, color = grp)) +
  geom_point(size = 3, alpha = 0.8) +
  theme_minimal() +
  labs(title = "NMDS of Coral Community Structure\nby Dist from Channel in NRC South",
       color = "Dist from channel")
```

Density and community composition are not different in the NRC S sites that are nearer vs. farther from the channel. So, no need to exclude the farther sites.


DCA dataset has the most sites in the "Artificial" habitat classification. (Some in other datasets too, but DCA has most). Can 'Artificial' be aggregated with 'Nearshore Ridge Complex'?

'Artificial' is only present in DCA and minorly in TT -- but absent from Shedd.   
It is in close spatial proximity to Nearshore Ridge Complex -- can these be combined?

Test for differences in coral density in DCA survey between habitat types
```{r artificial_vs_nrc_density}
# Subset DCA data
dcadf <- dca17_counts_ag %>%
  left_join(allsitemd) %>%
  filter(Type %in% c("Nearshore Ridge Complex", "Inner Reef", "Middle Reef", "Artificial", "Outer Reef", "Aggregated Patch Reef"))

# Fit a Negative Binomial GLM
mod_nb <- MASS::glm.nb(n ~ Type, data = dcadf)

# Generate new data only for existing taxon-size class combinations
newdata_1 <- dcadf %>%
  distinct(Type)

# Get predicted values & standard errors (log scale)
preds_nb <- predict(mod_nb, newdata_1, type = "link", se.fit = TRUE)

# Compute both total coral density & taxon-size class-specific densities in one step
results_nb <- newdata_1 %>%
  mutate(
    fit = exp(preds_nb$fit),                    # Convert fitted values to response scale
    fit_se = exp(preds_nb$fit) * preds_nb$se.fit,   # Convert SE using the Delta Method
    fit_var = (fit * preds_nb$se.fit)^2,         # Variance propagation
    fit_lower = exp(preds_nb$fit - 1.96 * preds_nb$se.fit),  # Lower CI
    fit_upper = exp(preds_nb$fit + 1.96 * preds_nb$se.fit)   # Upper CI
  )

# Compute total coral density + confidence intervals
total_ci_nb <- results_nb %>%
  group_by(Type) %>%
  summarize(
    total_density = sum(fit),
    total_se = sqrt(sum(fit_var)),
    lower_95CI = exp(log(total_density) - 1.96 * (total_se / total_density)),
    upper_95CI = exp(log(total_density) + 1.96 * (total_se / total_density))
  )

knitr::kable(total_ci_nb)
```
Highly overlapping confidence intervals for Artifical and Nearshore Ridge Complex, so no difference in coral density.

Test for differences in community composition in DCA survey
```{r artificial_vs_nrc_nmds}
library(vegan)

# 1. Create a wide community matrix: rows = site x Type, columns = taxa
comm_matrix <- dcadf %>%
  group_by(site, Type, taxon) %>%
  summarize(total_n = sum(n), .groups = "drop") %>%
  pivot_wider(names_from = taxon, values_from = total_n, values_fill = 0)

# 2. Extract community matrix and metadata
comm_data <- comm_matrix %>% select(-site, -Type)
site_info <- comm_matrix %>% select(site, Type)

# 3. Run NMDS (k = 2 dimensions is standard)
set.seed(123)  # for reproducibility
nmds <- metaMDS(comm_data, distance = "bray", k = 2, trymax = 100)

# 4. Prepare data for plotting
scores_df <- scores(nmds)$sites %>%
  bind_cols(site_info)

# 5. Plot NMDS with ggplot2
ggplot(scores_df, aes(x = NMDS1, y = NMDS2, color = Type)) +
  geom_point(size = 3, alpha = 0.8) +
  theme_minimal() +
  labs(title = "NMDS of Coral Community Structure by Habitat Type",
       color = "Habitat Type")
```

High degree of similarity between Artificial and Nearshore Ridge Complex coral communities.

Combine 'Artificial' with 'Nearshore Ridge Complex'
```{r combine_artificial_nrc}
# Based on these results, combine "Artificial" with "Nearshore Ridge Complex"
dff[dff$Type == "Artificial", "Type"] <- "Nearshore Ridge Complex"
```


Combine 'Aggregated Patch Reef' with 'Outer Reef', following previous studies (DCA)
```{r}
dff[dff$Type == "Aggregated Patch Reef", "Type"] <- "Outer Reef"

dff <- dff %>% mutate(Type = factor(Type, levels = type_levels))
```



## Summarize survey datasets
```{r}
# Count number of sites per habitat
nsites <- dff %>%
  group_by(dataset, Type) %>%
  summarize(n = n_distinct(site)) %>%
  mutate(Type = factor(Type, levels = type_levels)) %>%
  arrange(Type) %>%
  pivot_wider(names_from = Type, values_from = n, values_fill = 0) %>%
  janitor::adorn_totals(where = "col", name = "Total Sites")

# Get area surveyed per transect and total area surveyed
area_per_site <- dff %>%
  group_by(dataset) %>%
  summarise(
    area_per_site = {
      r <- range(transect_area_m2)
      if (r[1] == r[2]) as.character(r[1]) else paste(r, collapse = " — ")})

total_area <- dff %>%
  group_by(dataset, site) %>%
  summarize(max_area = max(transect_area_m2)) %>%
  summarize(total_area = sum(max_area))

# Get total number of corals counted
total_corals <- dff %>%
  group_by(dataset) %>%
  summarize(total_n = sum(n))

survey_summary <- reduce(
  list(nsites, area_per_site, total_area, total_corals),
  full_join,
  by = "dataset"
) %>%
  mutate(dataset = factor(dataset, levels = dataset_levels)) %>%
  arrange(dataset) %>% 
  janitor::adorn_totals(where = "row")

write_csv(survey_summary, file = "outputs/Table1.csv")

survey_summary %>% knitr::kable()
```

# Map datasets
```{r plot_maps, fig.width = 10, fig.height = 10}
(nsu11_esa_plot <- polyplot + 
  geom_point(
    data = filter(selected, dataset == "nsu11_esa"),
    aes(x = longitude, y = latitude), pch = 4, inherit.aes = FALSE, alpha = 0.6) +
  labs(title = "2011 — NSU ESA Survey"))

(dca17_esa_plot <- polyplot + 
  geom_point(
    data = filter(selected, dataset == "dca17_esa"),
    aes(x = longitude, y = latitude), pch = 4, inherit.aes = FALSE, alpha = 0.6) +
  labs(title = "2017 — DCA ESA Survey"))

(dca_plot <- polyplot + 
  geom_point(
    data = filter(selected, dataset == "dca17"),
    aes(x = longitude, y = latitude), pch = 4, inherit.aes = FALSE, alpha = 0.6) +
  labs(title = "2017 — DCA RECON Survey"))

(tt21_plot <- polyplot + 
  geom_point(
    data = filter(selected, dataset == "tt21"),
    aes(x = longitude, y = latitude), pch = 4, inherit.aes = FALSE, alpha = 0.6) +
  labs(title = "2021 — Tetra Tech Survey"))

(tt23_plot <- polyplot + 
  geom_point(
    data = filter(selected, dataset == "tt23"),
    aes(x = longitude, y = latitude), pch = 4, inherit.aes = FALSE, alpha = 0.6) +
  labs(title = "2023 — Tetra Tech Survey"))

(drm24_plot <- polyplot + 
  geom_point(
    data = filter(selected, dataset == "drm24"),
    aes(x = longitude, y = latitude), pch = 4, inherit.aes = FALSE, alpha = 0.6) +
  labs(title = "2024 — Shedd Survey"))



# One faceted plot
selected <- selected %>% mutate(dataset = factor(dataset, levels = c("nsu11_esa", "dca17_esa", "dca17", "tt21", "tt23", "drm24")))
all_surveys_plot <- polyplot +
  geom_point(
    data = selected,
    aes(x = longitude, y = latitude),
    pch = 4, alpha = 0.6, inherit.aes = FALSE
  ) +
  facet_wrap(~ dataset, ncol = 6) +
  labs(title = "Survey Site Locations by Dataset") +
  theme(legend.position = "none",
        plot.background = element_rect(fill = "white", color = NA),
        panel.background = element_rect(fill = "white", color = NA))

all_surveys_plot
ggsave(filename = "outputs/FigA1.png", width = 270, height = 220, units = "mm", dpi = 300)
```


# Estimate coral densities

## Model1 - all fixed effects

Predictors: dataset, habitat Type, direction from channel, taxon, size class

```{r brms}
# Just get columns we need for modeling
dfftaxon <- dff %>% select(dataset, Type, dir, site, transect_area_m2, taxon, class, n)

# Number of counts
nrow(dfftaxon)
# Number of zeros
table(dfftaxon$n == 0)

# Overdispersion
mean_count <- mean(dfftaxon$n)
var_count <- var(dfftaxon$n)
var_count / mean_count

# Number of taxa
n_distinct(dfftaxon$taxon)

# Number of sites
n_distinct(interaction(dfftaxon$dataset, dfftaxon$site))

# Number of corals
sum(dfftaxon$n)



# MODEL - FIT ON ALL DATA INCLUDING ALL STRUCTURAL ZEROS

# mod_nb <- brm(
#   bf(n ~ dataset * taxon:class * Type + taxon:class:Type:dir +
#        offset(log(transect_area_m2)),
#      zi ~ taxon:class * Type),
#   family = zero_inflated_negbinomial(),
#   data = dfftaxon,    # choose dataset with or without all structural zeros
#   prior = c(prior(normal(0, 2), class = "b"),          # Weak prior on coefficients
#             prior(normal(0, 5), class = "Intercept"),  # Weak prior on intercept
#             prior(exponential(1), class = "shape"),     # Reasonable prior for NB dispersion
#             prior(normal(0, 1), class = "b", dpar = "zi")),
#   chains = 4,
#   cores = 4,
#   threads = threading(5),
#   iter = 1000, warmup = 500,    #iter = 1000, warmup = 500,
#   thin = 1,
#   control = list(adapt_delta = 0.85, max_treedepth = 10),
#   backend = "cmdstanr"
# )
# saveRDS(mod_nb, file = "data/processed/mod_nb.rds")
mod_nb <- readRDS("data/processed/mod_nb.rds")


# 1. Create newdata grid (1 m² for standardization)
# Remove any taxa not observed in a given dataset / habitat Type ?????
dfftaxon_trimmed <- dfftaxon %>%
  group_by(dataset, Type, dir, taxon, class) %>%
  filter(any(n > 0)) %>%
  ungroup()
newdata <- dfftaxon_trimmed %>%       # USE DATA WITHOUT THE UNOBSERVED CORALS (DATASET/TYPE/DIR/TAXON/CLASS)
  distinct(dataset, Type, dir, taxon, class) %>%
  mutate(transect_area_m2 = 1)

# 2. Get fitted values manually by computing summary statistics across all draws
posterior_draws <- add_epred_draws(mod_nb, newdata = newdata,
                                   allow_new_levels = TRUE, re_formula = NA) %>%
  mutate(Type = factor(Type, levels = type_levels),
         dataset = factor(dataset, levels = dataset_levels))
```


### Taxon densities by dataset, habitat, and direction from channel
```{r taxon_density, fig.width = 10, fig.height = 10}
fitted_taxon <- posterior_draws %>%
  # Sum across size classes within taxa
  group_by(dataset, Type, dir, taxon, .draw) %>%
  summarise(
    epred_sum = sum(.epred),  # 👈 sum size classes
    .groups = "drop"
  ) %>%
  # Average across draws
  group_by(dataset, Type, dir, taxon) %>%
  summarise(
    fit_mean = mean(epred_sum),
    fit_sd = sd(epred_sum),
    fit_lower = quantile(epred_sum, 0.025),
    fit_upper = quantile(epred_sum, 0.975),
    .groups = "drop"
  )

# Plot
ggplot(fitted_taxon, aes(y = fit_mean, x = Type, color = dir, shape = dataset,
                          group = interaction(dataset, dir))) +
  facet_wrap(taxon ~ .) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_line(position = position_dodge(width = 0.5), alpha = 0.5) +
  geom_errorbar(aes(ymin = fit_lower, ymax = fit_upper), width = 0,
                position = position_dodge(width = 0.5), lwd = 0.25) +
  scale_y_log10()+#limits = c(1e-4, 2)) +
  scale_x_discrete(labels = type_labels) +
  labs(x = "Habitat Type", y = "Density (per m2)")
```

### North-south differences
```{r taxon_density_ns_diffs}
# Any N-S differences in taxon abundance within habitat Types/datasets?
#### Pivot wide so you can calculate N vs S difference per draw
taxon_NS_diff <- posterior_draws %>%
  ungroup() %>%
  select(.draw, dataset, Type, dir, taxon, class, .epred) %>%
  pivot_wider(names_from = dir, values_from = .epred) %>%
  filter(!is.na(N), !is.na(S)) %>%  # ensure both directions exist for the draw
  mutate(diff = S - N)  # or N - S depending on interpretation

taxon_NS_summ <- taxon_NS_diff %>%
  group_by(dataset, Type, taxon, class) %>%
  summarize(
    mean_diff = mean(diff),
    lower_95 = quantile(diff, 0.025),
    upper_95 = quantile(diff, 0.975),
    p_two_sided = 2 * min(mean(diff > 0), mean(diff < 0)),
    .groups = "drop"
  )

#table(p.adjust(taxon_NS_summ$p_two_sided, method = "holm") < 0.05)
# taxon_NS_summ[19,]
taxon_NS_summ %>%
  mutate(p_adj = p.adjust(p_two_sided, method = "bonferroni")) %>%
  filter(p_adj < 0.05) %>%
  mutate(greater = if_else(mean_diff > 0, "N", "S")) %>%
  group_by(dataset, Type, greater) %>%
  summarize(taxa = paste(taxon, class, collapse = ",")) %>%
  arrange(Type, greater) %>%
  knitr::kable()


taxon_NS_summ %>% filter(dataset == "dca17", Type == "Nearshore Ridge Complex", taxon == "MCAV", class == "<4cm")
```

A minority of taxon × size class × habitat × dataset combinations show a significant north-south effect.
The majority do not. they are dataset/habitat/taxon/size class-specific, and idiosyncratic.
My goal is not to focus on dir (direction) in the main analyses, but I want to account for it appropriately so it doesn’t confound other effects.
Let's move direction to a random effect. This approach acknowledges the variability without overfitting. It’s flexible and allows for occasional strong differences without forcing them to be estimated everywhere.

((When I changed FAVI adults to individual taxa and the formula in the first model to taxon:class in the fixed effects instead of taxon * class, the number of N-S differences dropped by a lot. probably because multiple comparisons numbers go up because more taxa.))

## Model2 - drop direction to RE
```{r}
# mod_nb2 <- brm(
#   bf(n ~ dataset * taxon:class * Type +
#        (1 | taxon:class:Type:dir) +
#        offset(log(transect_area_m2)),
#      zi ~ taxon:class * Type),
#   family = zero_inflated_negbinomial(),
#   data = dfftaxon,    # choose dataset with or without all structural zeros
#   prior = c(prior(normal(0, 2), class = "b"),          # Weak prior on coefficients
#             prior(normal(0, 5), class = "Intercept"),  # Weak prior on intercept
#             prior(exponential(1), class = "shape"),     # Reasonable prior for NB dispersion
#             prior(normal(0, 1), class = "b", dpar = "zi")),
#   chains = 4,
#   cores = 4,
#   threads = threading(5),
#   iter = 1000, warmup = 500,    #iter = 1000, warmup = 500,
#   thin = 1,
#   control = list(adapt_delta = 0.85, max_treedepth = 10),
#   backend = "cmdstanr"
# )
# saveRDS(mod_nb2, file = "data/processed/mod_nb2.rds")
mod_nb2 <- readRDS("data/processed/mod_nb2.rds")
```

```{r, eval = T}
loo_1 <- loo(mod_nb)
loo_2 <- loo(mod_nb2)

loo_compare(loo_1, loo_2)
# means second model with dir as random effect is better
```

### Taxon densities by dataset, habitat, and size class
```{r, fig.width = 10, fig.height = 10}
newdata2 <- dfftaxon_trimmed %>%       # USE DATA WITHOUT THE UNOBSERVED CORALS (DATASET/TYPE/DIR/TAXON/CLASS)
  distinct(dataset, Type, taxon, class) %>%
  mutate(transect_area_m2 = 1)
# 2. Get fitted values manually by computing summary statistics across all draws
posterior_draws2 <- add_epred_draws(mod_nb2, newdata = newdata2,
                                   allow_new_levels = TRUE, re_formula = NA) %>%
  mutate(Type = factor(Type, levels = type_levels),
         dataset = factor(dataset, levels = dataset_levels))

fitted_taxon2 <- posterior_draws2 %>%
  group_by(dataset, Type, taxon, class) %>%
  summarise(
    fit_mean = mean(.epred),
    fit_sd = sd(.epred),
    fit_lower = quantile(.epred, 0.025),
    fit_upper = quantile(.epred, 0.975),
    .groups = "drop"
  )

# Plot
ggplot(fitted_taxon2, aes(y = fit_mean, x = Type, shape = dataset,
                          group = interaction(dataset, class), color = class)) +
  facet_wrap(taxon ~ .) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_line(position = position_dodge(width = 0.5), alpha = 0.5) +
  geom_errorbar(aes(ymin = fit_lower, ymax = fit_upper), width = 0,
                position = position_dodge(width = 0.5), lwd = 0.25) +
  scale_y_log10()+#limits = c(1e-4, 2)) +
  scale_x_discrete(labels = type_labels) +
  labs(x = "Habitat Type", y = "Density (per m2)")
```


### Dataset/survey differences
```{r taxon_survey_diffs}
# Ensure dataset is character so we can do < comparison
draws_clean <- posterior_draws2 %>%
  mutate(dataset = as.character(dataset))

# One self-join to get all unique dataset pairs
pairwise_contrasts <- draws_clean %>%
  rename(dataset1 = dataset, epred1 = .epred) %>%
  inner_join(
    draws_clean %>% rename(dataset2 = dataset, epred2 = .epred),
    by = c(".draw", "taxon", "class", "Type")
  ) %>%
  filter(dataset1 < dataset2) %>%  # Avoid duplicates and self-pairs
  mutate(diff = epred1 - epred2)

# Step: summarize the posterior differences
summary_contrasts <- pairwise_contrasts %>%
  group_by(taxon, class, Type, dataset1, dataset2) %>%
  summarize(mean_diff = mean(diff),
            lower = quantile(diff, 0.025),
            upper = quantile(diff, 0.975),
            p_two_sided = 2 * min(mean(diff > 0), mean(diff < 0)),
            .groups = "drop")

summary_contrasts %>% 
  mutate(p_adj = p.adjust(p_two_sided, method = "bonferroni")) %>%
  filter(p_adj < 0.01) %>%
  arrange(taxon, Type) %>%
  knitr::kable()
```

Lots of idiosyncratic diffs between surveys for individual taxon x class groups? Reflects diff sites? No super clear patterns.


## Model3 - drop dataset to RE
```{r}
# 🎯 Goal
# 
# Estimate: density of taxon × class within each habitat Type
# ...averaging over datasets, but not letting datasets with more transects in a given habitat dominate unfairly.
# with zero-inflation that varies per taxon*class

mod_nb3 <- brm(
  bf(n ~ taxon:class * Type +
        (1 + Type | dataset) +
        (1 | dataset:taxon:class:Type:dir) +
        offset(log(transect_area_m2)),
     zi ~ taxon:class * Type),     
  data = dfftaxon,   # use dataset with all structural zeros present
  family = zero_inflated_negbinomial(),
  prior = c(
    prior(normal(0, 2), class = "b"),
    prior(normal(0, 5), class = "Intercept"),
    prior(exponential(1), class = "shape"),
    prior(normal(0, 1), class = "b", dpar = "zi")
  ),
  chains = 4, cores = 4, threads = threading(5),
  iter = 200, warmup = 50, thin = 1,
  backend = "cmdstanr",
  control = list(adapt_delta = 0.99),
  seed = 20240717
)
# 
# saveRDS(mod_nb3, file = "data/processed/mod_nb3.rds")
mod_nb3 <- readRDS("data/processed/mod_nb3.rds")
```

```{r}
mod_nb3_shape_by_taxon <- brm(
  bf(
    n ~ taxon:class * Type +
      (1 + Type | dataset) +
      (1 | dataset:taxon:class:Type:dir) +
      offset(log(transect_area_m2)),
    zi ~ taxon:class * Type,  #*Type      # your current ZI structure
    shape ~ taxon:class * Type #noType              # varying shape by taxon
  ),
  data = dfftaxon,
  family = zero_inflated_negbinomial(),
  prior = c(
    prior(normal(0, 2), class = "b"),
    prior(normal(0, 5), class = "Intercept"),
    prior(normal(0, 1), class = "b", dpar = "zi"),
    prior(normal(0, 1), class = "b", dpar = "shape")  # prior for varying shape
  ),
  chains = 4, cores = 4, threads = threading(5),
  iter = 200, warmup = 50, thin = 1,
  backend = "cmdstanr",
  control = list(adapt_delta = 0.99),
  seed = 20240717
)

```


### Model evaluation and comparisons
```{r}
summary(mod_nb3)  # Rhat should be ~1.00; no divergent transitions
rhat(mod_nb3)

loo_3_orig <- loo(mod_nb3_orig)
loo_3 <- loo(mod_nb3_shape_by_taxon)
loo_compare(loo_3_orig, loo_3)

loo_compare(loo_3, loo_2, loo_1)
loo_3
loo_2
loo_1

# Predictive performance similar between the two models (mod3 slightly better) -- can choose dataset as random (mod3) based on interpretability
# mod3 also has slightly lower p_loo (good, slightly simpler )

bayes_R2(mod_nb3)   
bayes_R2(mod_nb3_shape_by_taxon_plusType) 
bayes_R2(mod_nb3_shape_by_taxon)
```

```{r}
library(tidybayes)
library(dplyr)
library(ggplot2)
library(e1071)  # for skewness

# Step 1: Create taxon_class column
dfftaxon <- dfftaxon %>%
  mutate(taxon_class = paste(taxon, class, sep = "_"))

# Step 2: Draw predicted counts
ppreds <- add_predicted_draws(mod_nb3_shape_by_taxon, newdata = dfftaxon, ndraws = 600)

# Step 3: Summarize predicted statistics by taxon_class and draw
pp_stats <- ppreds %>%
  group_by(taxon_class, Type, .draw) %>%
  summarize(
    mean_pred = mean(.prediction),
    sd_pred = sd(.prediction),
    p_zero_pred = mean(.prediction == 0),
    skew_pred = e1071::skewness(.prediction),
    .groups = "drop"
  )

# Step 4: Summarize observed statistics
obs_stats <- dfftaxon %>%
  group_by(taxon_class, Type) %>%
  summarize(
    mean_obs = mean(n),
    sd_obs = sd(n),
    p_zero_obs = mean(n == 0),
    skew_obs = e1071::skewness(n),
    .groups = "drop"
  )

# Step 5: Plot function
plot_stat <- function(stat_name, stat_label) {
  ggplot(pp_stats, aes_string(x = "taxon_class", y = paste0(stat_name, "_pred"))) +
    stat_halfeye(fill = "skyblue", alpha = 0.6) +
    facet_wrap(~Type) +
    geom_point(data = obs_stats, aes_string(y = paste0(stat_name, "_obs")), color = "red", size = 2) +
    labs(
      y = stat_label,
      x = "Taxon and size class",
      title = paste("Observed vs predicted", tolower(stat_label), "by taxon:class")
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

# Step 6: Generate plots
plot_stat("mean", "Mean")+scale_y_continuous(trans = "log1p")
plot_stat("sd", "Standard Deviation") +scale_y_continuous(trans = "log1p")
plot_stat("p_zero", "Proportion Zeros")
plot_stat("skew", "Skewness")


```


```{r}

pp_check(mod_nb3_shape_by_taxon, type = "stat_grouped", group = "taxon", stat = "mean", ndraws = 600)
pp_check(mod_nb3_shape_by_taxon, type = "stat_grouped", group = "taxon", stat = "sd", ndraws = 600)
prop_zero <- function(x) mean(x == 0)
pp_check(mod_nb3_shape_by_taxon, type = "stat_grouped", group = "taxon", stat = "prop_zero", ndraws = 600)

library(moments)
# Posterior predictive simulations
yrep <- posterior_predict(mod_nb3_shape_by_taxon, ndraws = 600)

# Summary stats for each simulated dataset (one row = one posterior draw)
sim_summary <- data.frame(
  mean      = rowMeans(yrep),
  variance  = apply(yrep, 1, var),
  zero_prop = rowMeans(yrep == 0),
  skewness  = apply(yrep, 1, skewness)
)

# Observed data summaries
obs_n <- mod_nb3$data$n
obs_summary <- list(
  mean      = mean(obs_n),
  variance  = var(obs_n),
  zero_prop = mean(obs_n == 0),
  skewness  = skewness(obs_n)
)

# Compare observed to simulated
compare_stat <- function(stat_name) {
  cat(glue::glue(
    "\n{stat_name}:\n",
    "  Observed:     {round(obs_summary[[stat_name]], 2)}\n",
    "  Simulated:    Median = {round(median(sim_summary[[stat_name]]), 2)}, ",
    "IQR = ({round(quantile(sim_summary[[stat_name]], 0.25), 2)}, {round(quantile(sim_summary[[stat_name]], 0.75), 2)})\n"
  ))
}

# Run comparisons
compare_stat("mean")
compare_stat("variance")
compare_stat("zero_prop")
compare_stat("skewness")


p1 <- pp_check(mod_nb3, type = "stat", stat = "mean", ndraws = 600)
p2 <- pp_check(mod_nb3, type = "stat", stat = "var", ndraws = 600)
prop_zero <- function(x) mean(x == 0)
p3 <- pp_check(mod_nb3, type = "stat", stat = "prop_zero", ndraws = 600)
p4 <- pp_check(mod_nb3, type = "stat", stat = "skewness", ndraws = 600)

cowplot::plot_grid(p1, p2, p3, p4, nrow = 2)

ggsave(filename = "outputs/modeleval.png")
```

### Final density estimates
```{r}
# fixed version of equally weighting estimates across datasets
# Step 1: Trim to only taxon–class–habitat combinations where the taxon was observed at least once
dfftaxon_trimmed2 <- dfftaxon %>%
  semi_join(
    dfftaxon %>%
      group_by(taxon, class, Type) %>%
      filter(any(n > 0)) %>%
      ungroup() %>%
      distinct(taxon, class, Type),
    by = c("taxon", "class", "Type")
  )

# Step 2: Create newdata for prediction — includes all unique combinations, setting area = 1 m²
newdata <- dfftaxon_trimmed2 %>%
  distinct(dataset, taxon, class, Type, dir) %>%
  mutate(transect_area_m2 = 1)

# Step 3: Get population-level predictions from the model (includes all group-level effects)
preds <- add_epred_draws(mod_nb3_shape_by_taxon, newdata = newdata, re_formula = NULL)

# Step 4: Average across dirs within each dataset to prevent over-weighting datasets with both N/S
draws_avg_dir <- preds %>%
  group_by(.draw, taxon, class, Type, dataset) %>%
  summarize(n_m2 = mean(.epred), .groups = "drop")

# Step 5: Average equally across datasets — each dataset contributes one value
draws_equal_weighted <- draws_avg_dir %>%
  group_by(.draw, taxon, class, Type) %>%
  summarize(n_m2 = mean(n_m2), .groups = "drop")

# Step 6: Summarize posterior draws — mean and 95% credible interval
summary_equal_weighted <- draws_equal_weighted %>%
  group_by(taxon, class, Type) %>%
  mean_qi(n_m2, .width = 0.95)
```

# Results

## Densities

### Taxon densities
```{r, fig.width = 10, fig.height = 10}
###############
# DENSITY OF EACH TAXON x CLASS IN EACH HABITAT TYPE

# Order taxa by overall abundance for plotting
taxorder <- summary_equal_weighted %>%
  complete(taxon, class, Type, fill = list(n_m2 = 0)) %>%
  group_by(taxon, Type) %>%
  summarize(totdens = sum(n_m2)) %>%
  group_by(taxon) %>%
  summarize(meandens = mean(totdens)) %>%
  mutate(taxon = fct_reorder(taxon, -meandens))

summary_equal_weighted <- summary_equal_weighted %>%
  mutate(taxon = factor(taxon, levels = levels(taxorder$taxon)))

#### PLOT
(taxdens <- ggplot(summary_equal_weighted, aes(x = Type, y = n_m2, shape = class)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0, linewidth = 0.25,
                position = position_dodge(width = 0.2)) +
  geom_line(aes(group = class, linetype = class),
             position = position_dodge(width = 0.2)) +
  geom_point(size = 1.5, color = "black", fill = "white",
              position = position_dodge(width = 0.2)) +
  facet_wrap(~taxon, labeller = labeller(taxon = as_labeller(taxon_labels, label_parsed)), scales = "free_y", ncol = 4) +
  scale_shape_manual(values = c(21, 24)) +
  scale_x_discrete(labels = type_labels) +
  scale_linetype_manual(values = c(2,1)) +
  labs(x = "Habitat Type", y = "Density (per m2)")+
  theme_minimal())


taxon_group_df <- bind_rows(lapply(names(taxon_groups_juv), function(group) {
  tibble(taxon = taxon_groups_juv[[group]], group = group)
}))
taxon_group_df <- taxon_group_df %>% filter(taxon != group)

annotation_df <- summary_equal_weighted %>%
  distinct(taxon) %>%
  left_join(taxon_group_df, by = "taxon") %>%
  filter(!is.na(group)) %>%
  mutate(group2 = case_when(group == "FAVI" ~ "Faviinae",
                            group == "MUSS" ~ "Mussinae",
                            group == "MEAN" ~ "Meandrinidae")) %>%
  mutate(taxon = factor(taxon, levels = taxorder$taxon),
         label = paste0("<4cm = ", group2),
         # Choose a y-position (adjust as needed for your scale)
         x = 2.5,
         y = Inf)  # log10-scale safe low value

fig2 <- taxdens +
  geom_text(data = annotation_df, aes(x = x, y = y, label = label),
            inherit.aes = FALSE, hjust = 0.5, size = 2.5, color = "black") +
  theme(panel.background = element_rect(fill = "white", color = NA),
        plot.background = element_rect(fill = "white", color = NA)) +
  coord_cartesian(clip = "off") +
  scale_y_continuous(limits = c(0, NA)) +
  labs(linetype = "Size class") +
  labs(shape = "Size class") +
  theme(legend.position = "bottom",
        legend.key.width = unit(2, "lines"),
        legend.margin = margin(t = -5, b = 0))  # move legend up)
fig2
ggsave(filename = "outputs/Fig2new.png", width = 170, height = 170, units = "mm", dpi = 300)

#### TABLE

# Step 1: Format the data
type_order <- unique(summary_equal_weighted$Type)  # preserves original order
class_order <- c("<4cm", ">4cm")

summary_preds_fmt <- summary_equal_weighted %>%
  mutate(
    class = factor(class, levels = class_order),
    Type = factor(Type, levels = type_order),
    mean_ci = case_when(
      n_m2 < 0.001 ~ sprintf("%.1e<br>[%.1e–%.1e]", n_m2, .lower, .upper),
      TRUE ~ sprintf("%.4f<br>[%.4f–%.4f]", n_m2, .lower, .upper)
    )
  )

# Step 2: Pivot to have one column per habitat Type
table_out <- summary_preds_fmt %>%
  select(taxon, class, Type, mean_ci) %>%
  pivot_wider(names_from = Type, values_from = mean_ci) %>%
  arrange(taxon, class)

# Step 3: Pretty-print the table
knitr::kable(table_out, format = "html", escape = FALSE,
             caption = "Density (per m2) of each coral taxon and size class in each habitat type, with 95% C.I.") %>%
  kable_styling(font_size = 10)


############
write_csv(table_out, file = "outputs/TableA1.csv")
```

### Total densities by size and SCTLD-susceptibility groups

```{r}
type_labels2 <- c("Nearshore\nRidge Complex", "Inner\nReef", "Middle\nReef", "Outer\nReef")
# ---- TOTAL CORAL DENSITY ----
summary_total_density_by_type <- draws_equal_weighted %>%
  group_by(.draw, Type) %>%
  summarize(total_density = sum(n_m2), .groups = "drop") %>%
  group_by(Type) %>%
  summarize(
    mean = mean(total_density),
    lower = quantile(total_density, 0.025),
    upper = quantile(total_density, 0.975),
    .groups = "drop"
  )

plot_total <- ggplot(summary_total_density_by_type, aes(x = Type, y = mean)) +
  geom_col(fill = "gray60") +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, linewidth = 0.25) +
  scale_x_discrete(labels = type_labels2) +
  scale_y_continuous(limits = c(0, 2.6), breaks = seq(0, 2.5, 0.5), minor_breaks = seq(0, 2.6, 0.25)) +
  labs(
    x = "Habitat Type",
    y = "Density (per m²)",
    title = NULL
  ) +
  theme_minimal(base_size = 8) +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 0.5),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA)
  )

# ---- BY SIZE CLASS ----
summary_class <- draws_equal_weighted %>%
  group_by(.draw, Type, class) %>%
  summarize(total_epred = sum(n_m2), .groups = "drop") %>%
  group_by(Type, class) %>%
  mean_qi(total_epred, .width = 0.95) %>%
  mutate(
    class = factor(class, levels = c("<4cm", ">4cm")),
    label = sprintf("%.2f", total_epred)
  )

plot_class <- ggplot(summary_class, aes(x = Type, y = total_epred, fill = class)) +
  geom_col(position = position_dodge(width = 0.8), width = 0.8) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper),
                position = position_dodge(width = 0.8), width = 0.2, linewidth = 0.25) +
  #geom_text(aes(label = label), position = position_dodge(width = 0.8), vjust = -0.5, size = 2.5) +
  scale_fill_manual(values = c(">4cm" = "gray60", "<4cm" = "gray75")) +
  scale_x_discrete(labels = type_labels2) +
  scale_y_continuous(limits = c(0, 2.6), breaks = seq(0, 2.5, 0.5), minor_breaks = seq(0, 2.6, 0.25)) +
  labs(
    x = "Habitat Type",
    y = "",
    fill = "Size Class"
  ) +
  theme_minimal(base_size = 8) +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 0.5),
    legend.key.size = unit(4, "mm"),
    legend.background = element_rect(fill = "white", color = "black"),
    legend.position = c(0.7, 0.85),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA)
  )

# ---- BY SUSCEPTIBILITY GROUP ----
# Define susceptibility groups
group_definitions <- tibble(
  taxon = unique(preds$taxon),
  sus_group = case_when(
    taxon %in% c("FAVI", "MUSS", "MEAN", "DSTO", "EFAS", "MMEA", "DLAB", "CNAT", "PSTR", "PCLI") ~ "high_sus",
    taxon %in% c("MCAV", "ORBI", "SIDE", "SINT", "SOLE") ~ "med_sus",
    taxon %in% c("ACER", "PORI", "AGAR", "MYCE", "MADR") ~ "low_sus",
    TRUE ~ NA_character_
  )
) %>%
  mutate(sus_group = factor(sus_group, levels = c("low_sus", "med_sus", "high_sus")))

summary_sus <- draws_equal_weighted %>%
  left_join(group_definitions) %>%
  group_by(.draw, Type, sus_group) %>%
  summarize(total_epred = sum(n_m2), .groups = "drop") %>%
  group_by(Type, sus_group) %>%
  mean_qi(total_epred, .width = 0.95) %>%
  mutate(
    taxon_group = factor(sus_group, levels = c("high_sus", "med_sus", "low_sus")),
    label = sprintf("%.2f", total_epred)
  )

plot_sus <- ggplot(summary_sus, aes(x = Type, y = total_epred, fill = sus_group)) +
  geom_col(position = position_dodge(width = 0.8), width = 0.8) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper),
                position = position_dodge(width = 0.8), width = 0.2, linewidth = 0.25) +
  #geom_text(aes(label = label), position = position_dodge(width = 0.8), vjust = -0.5, size = 2.5) +
  scale_fill_manual(
    values = c("high_sus" = "gray60", "med_sus" = "gray75", "low_sus" = "gray90"),
    labels = c("high_sus" = "High", "med_sus" = "Medium", "low_sus" = "Low")
  ) +
  scale_x_discrete(labels = type_labels2) +
  scale_y_continuous(limits = c(0, 2.6), breaks = seq(0, 2.5, 0.5), minor_breaks = seq(0, 2.6, 0.25)) +
  labs(
    x = "Habitat Type",
    y = "",
    fill = "SCTLD Susceptibility"
  ) +
  theme_minimal(base_size = 8) +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 0.5),
    legend.key.size = unit(4, "mm"),
    legend.background = element_rect(fill = "white", color = "black"),
    legend.position = c(0.7, 0.85),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA)
  )

# ---- Combine using cowplot ----
combined_plot <- cowplot::plot_grid(
  plot_total, plot_class, plot_sus,
  ncol = 3,
  labels = c("A", "B", "C"),
  label_size = 10,
  align = "hv"
)

combined_plot
ggsave(filename = "outputs/Fig3.png", width = 180, height = 80, units = "mm", dpi = 300)
```

## Abundances in Impact Zones

### Taxon abundances
```{r}
# TOTAL COLONIES IN EACH IMPACT ZONE

# Calculate total per taxclass per habitat Type and impact zone
tot1 <- left_join(summary_equal_weighted, area_summary, by = "Type") %>%
  mutate(tot = n_m2 * total_area_m2,
         tot_lower = n_m2 * total_area_m2,
         tot_upper = n_m2 * total_area_m2)

# taba2 <- tot1 %>%
#   group_by(Type, name) %>%
#   summarize(total_area_m2 = sum(total_area_m2), .groups = "drop") %>%
#   pivot_wider(names_from = name, values_from = total_area_m2) %>%
#   janitor::adorn_totals(where = c("row", "col")) %>%
#   mutate(across(where(is.numeric), ~ formatC(.x, format = "e", digits = 2)))
# 
# taba2 %>%
#   knitr::kable(caption = "Total area (m2) of each habitat type in each impact zone")
# 
# # Transpose using first column as rownames
# taba2_flipped <- taba2 %>%
#   column_to_rownames(var = names(taba2)[1]) %>%  # Set first column as rownames
#   t() %>%
#   as.data.frame() %>%
#   rownames_to_column(var = names(taba2)[1])      # Move former colnames into a new column
# 
# write_csv(taba2_flipped, file = "outputs/TableA2.csv")

# Calculate total per taxclass per impact zone (sum all habitat Types)
tot2 <- tot1 %>%
  group_by(taxon, class, name) %>%
  summarize(tot = round(sum(tot))) %>%
  arrange(name)

tab2 <- tot2 %>% ungroup() %>%
  pivot_wider(names_from = name, values_from = tot, names_sort = FALSE) %>%
  janitor::adorn_totals(where = c("row", "col")) %>%
  mutate(across(where(is.numeric), scales::comma))

tab2 %>%
  knitr::kable(caption = "Total number of colonies of each coral taxon and size class in each impact zone")

write_csv(tab2, file = "outputs/TableA2.csv")
```

### Total abundances
```{r}
# Calculate total ESA and non-ESA corals per impact zone
tot3 <- tot2 %>%
  mutate(ESA = if_else(taxon %in% c("ACER", "ORBI"), "ESA", "non-ESA")) %>%
  group_by(name, ESA) %>%
  summarize(tot = sum(tot), .groups = "drop") %>%
  pivot_wider(names_from = ESA, values_from = tot) %>%
  janitor::adorn_totals(where = "col") %>%
  mutate(cum_ESA = cumsum(ESA),
         cum_non_ESA = cumsum(`non-ESA`),
         cum_Total = cumsum(Total)) %>%
  mutate(across(where(is.numeric), ~ format(round(.), big.mark = ","))) 

tot3 %>%
  knitr::kable(caption = "Total coral colonies in all impact zones")

write_csv(tot3, file = "outputs/Table3.csv")

ttt <- tot2 %>% filter(taxon %in% c("ACER", "ORBI")) %>%
  group_by(taxon) %>%
  summarize(tot = sum(tot))
ttt
```


```{r}
posterior_type_totals <- draws_avg_dir %>%
  group_by(.draw, taxon, class, Type, dataset) %>%
  summarize(mean_value = mean(n_m2), .groups = "drop") %>%
  group_by(.draw, dataset, Type) %>%
  summarize(total_epred = sum(mean_value), .groups = "drop")


total_perType <- posterior_type_totals %>%
  group_by(dataset, Type) %>%
  summarize(
    fit_mean = mean(total_epred),
    fit_sd = sd(total_epred),
    fit_lower = quantile(total_epred, 0.025),
    fit_upper = quantile(total_epred, 0.975),
    .groups = "drop"
  )

totals <- left_join(total_perType, area_summary, by = "Type") %>%  # area_summary must have Type, total_area_m2, name
  mutate(
    tot_estimate = fit_mean * total_area_m2,
    tot_lower = fit_lower * total_area_m2,
    tot_upper = fit_upper * total_area_m2,
    Type = factor(Type, levels = type_levels),
    name = factor(name, levels = zone_levels)
  ) %>%
  complete(dataset, Type, fill = list(tot_estimate = NA, tot_lower = NA, tot_upper = NA))

totals <- totals %>% mutate(dataset = factor(dataset, levels = dataset_levels))

ggplot(totals, aes(x = dataset, y = tot_estimate, fill = fct_rev(name))) +
  facet_grid(~Type) +
  geom_col(position = "stack") +
  scale_fill_brewer(palette = "Set3") +
  labs(
    x = "Survey Dataset",
    y = "Total Corals (area-scaled)",
    fill = "Impact Zone",
    title = "Estimated Total Coral Abundance per Habitat Type and Dataset"
  ) +
  theme_classic(base_size = 10) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5),
    strip.background = element_blank(),
    strip.text = element_text(face = "bold")
  )

ggsave(filename = "outputs/FigA2.png")
```



### Treemap plot - Scenario2+NMFS
```{r working_treemap1}
# STEP 1: Aggregate total abundance by taxon + class
tot4 <- tot2 %>%
  filter(name != "Rest of Monitoring Area") %>%
  group_by(taxon, class) %>%
  summarize(tot = sum(tot), .groups = "drop")

# Assign a base color per taxon
set.seed(3)
base_colors <- sample(ggsci::pal_d3("category20")(20), 20)
names(base_colors) <- unique(tot4$taxon)

# Define shading function
get_shades <- function(base_col, class, all_classes) {
  if (length(all_classes) == 2) {
    if (class == "<4cm") colorspace::lighten(base_col, 0.1)
    else colorspace::darken(base_col, 0.1)
  } else {
    base_col
  }
}

# Build taxon_class and corresponding shades
shade_map <- tot4 %>%
  distinct(taxon, class) %>%
  rowwise() %>%
  mutate(
    base_col = base_colors[[taxon]],
    all_classes = list(tot4$class[tot4$taxon == taxon]),
    shade = get_shades(base_col, class, all_classes),
    taxon_class = paste(taxon, class, sep = "_")
  ) %>%
  ungroup()

# Final color map
fill_colors <- setNames(shade_map$shade, shade_map$taxon_class)

# Add taxon_class to original data
tot4 <- tot4 %>%
  mutate(taxon_class = paste(taxon, class, sep = "_"))

# Plot with taxon labels only
p <- ggplot(tot4, aes(area = tot, fill = taxon_class, subgroup = taxon)) +
  geom_treemap(color = NA) +
  geom_treemap_subgroup_border(colour = "white", size = 1) +
  # geom_treemap_subgroup_text(place = "topleft", grow = TRUE, 
  #                             alpha = 0.8, colour = "black", size = 8) +
  scale_fill_manual(values = fill_colors) +
  theme(legend.position = "none")


# 5. Get tile layout and attach taxon_class
tile_layout <- treemapify(tot4, area = "tot", 
                          fill = "taxon_class", subgroup = "taxon")

# 6. Flag small boxes (aggregate size classes)
tile_layout_ag <- tile_layout %>%
  group_by(taxon) %>%
  summarize(xmin = min(xmin), xmax = max(xmax),
            ymin = min(ymin), ymax = max(ymax),
            area = (xmax - xmin) * (ymax - ymin),
            center_x = (xmin + xmax) / 2,
            center_y = (ymin + ymax) / 2)
area_threshold <- 0.001
internal_labels <- tile_layout_ag %>% 
  filter(area >= area_threshold)
external_labels <- tile_layout_ag %>% 
  filter(area < area_threshold)

# Create labels with taxontotals
taxtots <- tot4 %>%
  group_by(taxon) %>%
  summarize(tot = scales::comma(sum(tot)))

internal_labels <- left_join(internal_labels, taxtots) %>%
  mutate(label = paste0(taxon, "\n", tot))

external_labels <- left_join(external_labels, taxtots) %>%
  mutate(label = paste0(taxon, "\n", tot))

# 7. Final plot
p + 
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  coord_cartesian(clip = "off") +
  geom_text(data = internal_labels,
            aes(x = xmin, y = ymax-0.00001,
                label = label, size = area), inherit.aes = FALSE,
            hjust = 0, vjust = 1, lineheight = 0.7,
            nudge_x = 0.001, nudge_y = -0.005) +
  scale_size_continuous(range = c(1.5, 15)) +
  geom_text_repel(data = external_labels,
             aes(x = center_x, y = center_y, 
                 label = label), inherit.aes = FALSE,
             max.overlaps = Inf,
             xlim = c(1, NA),
             force = 5,
             min.segment.length = 0.1,
             segment.size = 0.2,
             hjust = 0,
             size = 2) +
  theme_void() +
  theme(
    legend.position = "none",
    plot.margin = margin(t = 0, r = 20, b = 10, l = 10),
    plot.background = element_rect(fill = "white", color = NA),
  )
  
ggsave(filename = "outputs/Fig4.png", width = 180, height = 130, units = "mm", dpi = 600)

sum(tot4$tot)
tot4 %>% filter(taxon %in% c("ACER", "ORBI")) %>% summarize(tot = sum(tot))

tot2 %>% group_by(taxon) %>% summarize(tot = sum(tot)) %>% filter(taxon %in% c("ACER", "ORBI"))
```

### Treemap plot - Full monitoring area
```{r working_treemap2}
# STEP 1: Aggregate total abundance by taxon + class
tot4 <- tot2 %>%
  group_by(taxon, class) %>%
  summarize(tot = sum(tot), .groups = "drop")

# Assign a base color per taxon
set.seed(3)
base_colors <- sample(ggsci::pal_d3("category20")(20), 20)
names(base_colors) <- unique(tot4$taxon)

# Define shading function
get_shades <- function(base_col, class, all_classes) {
  if (length(all_classes) == 2) {
    if (class == "<4cm") colorspace::lighten(base_col, 0.1)
    else colorspace::darken(base_col, 0.1)
  } else {
    base_col
  }
}

# Build taxon_class and corresponding shades
shade_map <- tot4 %>%
  distinct(taxon, class) %>%
  rowwise() %>%
  mutate(
    base_col = base_colors[[taxon]],
    all_classes = list(tot4$class[tot4$taxon == taxon]),
    shade = get_shades(base_col, class, all_classes),
    taxon_class = paste(taxon, class, sep = "_")
  ) %>%
  ungroup()

# Final color map
fill_colors <- setNames(shade_map$shade, shade_map$taxon_class)

# Add taxon_class to original data
tot4 <- tot4 %>%
  mutate(taxon_class = paste(taxon, class, sep = "_"))

# Plot with taxon labels only
p <- ggplot(tot4, aes(area = tot, fill = taxon_class, subgroup = taxon)) +
  geom_treemap(color = NA) +
  geom_treemap_subgroup_border(colour = "white", size = 1) +
  # geom_treemap_subgroup_text(place = "topleft", grow = TRUE, 
  #                             alpha = 0.8, colour = "black", size = 8) +
  scale_fill_manual(values = fill_colors) +
  theme(legend.position = "none")


# 5. Get tile layout and attach taxon_class
tile_layout <- treemapify(tot4, area = "tot", 
                          fill = "taxon_class", subgroup = "taxon")

# 6. Flag small boxes (aggregate size classes)
tile_layout_ag <- tile_layout %>%
  group_by(taxon) %>%
  summarize(xmin = min(xmin), xmax = max(xmax),
            ymin = min(ymin), ymax = max(ymax),
            area = (xmax - xmin) * (ymax - ymin),
            center_x = (xmin + xmax) / 2,
            center_y = (ymin + ymax) / 2)
area_threshold <- 0.001
internal_labels <- tile_layout_ag %>% 
  filter(area >= area_threshold)
external_labels <- tile_layout_ag %>% 
  filter(area < area_threshold)

# Create labels with taxontotals
taxtots <- tot4 %>%
  group_by(taxon) %>%
  summarize(tot = scales::comma(sum(tot)))

internal_labels <- left_join(internal_labels, taxtots) %>%
  mutate(label = paste0(taxon, "\n", tot))

external_labels <- left_join(external_labels, taxtots) %>%
  mutate(label = paste0(taxon, "\n", tot))

# 7. Final plot
p + 
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  coord_cartesian(clip = "off") +
  geom_text(data = internal_labels,
            aes(x = xmin, y = ymax-0.00001,
                label = label, size = area), inherit.aes = FALSE,
            hjust = 0, vjust = 1, lineheight = 0.7,
            nudge_x = 0.005, nudge_y = -0.005) +
  scale_size_continuous(range = c(1.5, 15)) +
  geom_text_repel(data = external_labels,
             aes(x = center_x, y = center_y, 
                 label = label), inherit.aes = FALSE,
             max.overlaps = Inf,
             xlim = c(1, NA),
             force = 5,
             min.segment.length = 0.1,
             segment.size = 0.2,
             hjust = 0,
             size = 2) +
  theme_void() +
  theme(
    legend.position = "none",
    plot.margin = margin(t = 0, r = 20, b = 10, l = 10),
    plot.background = element_rect(fill = "white", color = NA),
  )
  
ggsave(filename = "outputs/FigA3.png", width = 180, height = 130, units = "mm", dpi = 600)

sum(tot4$tot)
tot4 %>% filter(taxon %in% c("ACER", "ORBI")) %>% summarize(tot = sum(tot))
```




########## spatially-explicit ACER analysis?
```{r}

```



## Temporal trends across datasets

### Taxon trends

```{r, include = T, eval = T, fig.width = 10, fig.height = 6}
# Step 1: Create newdata (retain dataset, taxon, class, and Type)
newdata_taxon <- dfftaxon_trimmed2 %>%
  distinct(dataset, taxon, class, Type) %>%
  mutate(transect_area_m2 = 1, dir = NA)

# Step 2: Posterior predictions including group-level effects
preds_taxon <- add_epred_draws(
  mod_nb3,
  newdata = newdata_taxon,
  re_formula = NULL,
  allow_new_levels = TRUE
)

# are datasets generating fairly comparable estimates for the abundant taxa within habitat types?
# preds_taxon %>%
#   filter(taxon %in% c("SIDE", "PORI", "SINT", "MCAV")) %>%
#   group_by(.draw, taxon, dataset, Type) %>%
#   summarize(n_m2 = sum(.epred), .groups = "drop") %>%
#   group_by(taxon, dataset, Type) %>%
#   mean_qi(n_m2, .width = 0.95) %>%
#   ggplot(aes(x = dataset, y = n_m2)) +
#   geom_point() +
#   geom_errorbar(aes(ymin = .lower, ymax = .upper)) +
#   facet_grid(taxon~Type, scales = "free_y")
# yes

# Step 3: Average across habitat types per draw, dataset, taxon, class
preds_avg <- preds_taxon %>%
  group_by(.draw, dataset, taxon, class) %>%
  summarize(mean_epred = mean(.epred), .groups = "drop")


# Step 4: Sum across size classes to get total density per taxon
preds_total <- preds_avg %>%
  group_by(.draw, dataset, taxon) %>%
  summarize(total_epred = sum(mean_epred), .groups = "drop")

# Step 5: Summarize posterior distribution
summary_taxon_total <- preds_total %>%
  group_by(dataset, taxon) %>%
  mean_qi(total_epred, .width = 0.95)

# Optional: Order datasets
summary_taxon_total <- summary_taxon_total %>%
  mutate(dataset = factor(dataset, levels = c("nsu11_esa", "dca17_esa", "dca17", "tt21", "tt23", "drm24"))) %>%
  mutate(year = parse_number(as.character(dataset)))

# Step 6: Plot
ggplot(summary_taxon_total, aes(x = year, y = total_epred)) +
  geom_line(aes(group = taxon), linewidth = 0.6, color = "gray40") +
  geom_point(aes(shape = dataset), size = 2.5, color = "black") +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.1) +
  #geom_smooth(method = "loess", se = FALSE, lty = 2) +
  facet_wrap(~taxon, scales = "free_y") +
  labs(
    x = "Year",
    y = "Total Coral Density (per m²)",
    title = "Total Coral Density per Taxon Across Datasets"
  ) +
  #scale_y_log10()+
  scale_x_continuous(breaks = seq(11,24,4), labels = paste0("'", seq(11,24,4))) +
  scale_shape_manual(values = c(1,0,7,2,5,6))+
  theme_minimal(base_size = 11) +
  theme(plot.background = element_rect(fill = "white", color = NA),
        legend.position = "bottom")

ggsave(filename = "outputs/FigA4.png", width = 180, height = 150, units = "mm", dpi = 300)



mod <- lm(total_epred ~ year * taxon, data = summary_taxon_total %>% filter(year != 11))
library(emmeans)
slopes_emm <- emtrends(mod, ~ taxon, var = "year")
summary(slopes_emm, infer = TRUE)  # Includes estimate, SE, and p-value for each slope

# SIDE has increased over time (2017-2024), but that's the only significant slope
```

### Total trends and SCTLD susceptibility
```{r}
# TOTAL coral density over time/datasets

# Step 4: Sum total predicted density (all taxa and size classes)
summary_total <- preds_avg %>%
  group_by(.draw, dataset) %>%
  summarize(total_density = sum(mean_epred), .groups = "drop") %>%
  group_by(dataset) %>%
  mean_qi(total_density, .width = 0.95)

# Step 5: Add year and filter datasets if needed
summary_total_filtered <- summary_total %>%
  filter(dataset %in% c("dca17", "tt21", "tt23", "drm24")) %>%
  mutate(dataset = factor(dataset, levels = c("dca17", "tt21", "tt23", "drm24")),
         year = parse_number(as.character(dataset)))





# Step 5: Join susceptibility group
preds_grouped <- preds_avg %>%
  left_join(group_definitions, by = "taxon") %>%
  filter(!is.na(sus_group))

# Step 6: Sum total predicted density per susceptibility group (all taxa, class in group)
summary_sus <- preds_grouped %>%
  group_by(.draw, sus_group, dataset) %>%
  summarize(total_density = sum(mean_epred), .groups = "drop") %>%
  group_by(sus_group, dataset) %>%
  mean_qi(total_density, .width = 0.95)

# Exclude unwanted datasets and set dataset order
summary_sus_filtered <- summary_sus %>%
  filter(dataset %in% c("dca17", "tt21", "tt23", "drm24")) %>%
  mutate(dataset = factor(dataset, levels = c("dca17", "tt21", "tt23", "drm24")),
         year = parse_number(as.character(dataset)))




# Step 2: Combine total with susceptibility group summaries
combined <- bind_rows(
  summary_total_filtered %>% select(dataset, year, total_density, .lower, .upper) %>% mutate(sus_group = "total"),
  summary_sus_filtered %>% rename(total_density = total_density)
)

combined$sus_group <- factor(combined$sus_group,
                             levels = c("total", "low_sus", "med_sus", "high_sus"))



lm_stats_full <- combined %>%
  group_by(sus_group) %>%
  do({
    model <- lm(total_density ~ year, data = .)
    coef <- coef(model)
    slope_pval <- coef(summary(model))["year", "Pr(>|t|)"]
    tibble(intercept = coef[1], slope = coef[2], pval = slope_pval)
  })%>%
  ungroup() %>%
  mutate(
    pred_2017 = intercept + slope * 17,
    pred_2024 = intercept + slope * 24,
    percent_change = 100 * (pred_2024 - pred_2017) / pred_2017,
    label = paste0("Trend = +", signif(percent_change, 3), "%", "\np = ", round(pval, 2))
  )

# View the results
lm_stats_full %>% select(sus_group, percent_change, label)

# Define annotation positions per panel
lm_labels2 <- combined %>%
  group_by(sus_group) %>%
  summarize(
    x = min(year),
    y = max(total_density),
    .groups = "drop"
  ) %>%
  left_join(lm_stats_full %>% select(sus_group, label), by = "sus_group")


# Step 4: Plot all 4 panels
changeplot <- ggplot(combined, aes(x = year, y = total_density, shape = dataset)) +
  geom_line(linewidth = 0.5, aes(group = sus_group), color = "black") +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.1, linewidth = 0.25) +
  geom_point(size = 3, fill = "white") +
  geom_smooth(
    aes(group = sus_group),
    method = "lm",
    se = FALSE,
    color = "blue"
  ) +
  facet_wrap(~sus_group, scales = "free", nrow = 1,
             labeller = as_labeller(c(
               "total" = "A. All corals",
               "low_sus" = "B. Low SCTLD-susc.",
               "med_sus" = "C. Moderate SCTLD-susc.",
               "high_sus" = "D. High SCTLD-susc."
             ))) +
  labs(
    x = "Year",
    y = "Mean Coral Density (per m²)",
    shape = "Dataset"
  ) +
  scale_shape_manual(values = c(21, 22, 24, 25)) +
  theme_classic(base_size = 10) +
  theme(
    strip.background = element_blank(),
    strip.text = element_text(hjust = 0, face = "bold", margin = margin(l = -15, b = 10), size = 10),
    legend.position = "bottom",
    legend.margin = margin(t = 0, r = 0, b = 0, l = 0),
    legend.box.margin = margin(t = -5, r = 0, b = 0, l = 0)
  ) +
  geom_text(
    data = lm_labels2,
    aes(x = x, y = Inf, label = label),
    hjust = 0, vjust = 1.2, size = 3, color = "blue",
    inherit.aes = FALSE
  ) +
  coord_cartesian(clip = "off")
changeplot
ggsave(filename = "outputs/Fig5.png", width = 180, height = 80, units = "mm")
```

